{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aab9de6-9bfc-4e3c-80c4-018778f6a47d",
   "metadata": {
    "id": "6aab9de6-9bfc-4e3c-80c4-018778f6a47d"
   },
   "source": [
    "# PyTorch Global, Unstructured & Iterative Pruning:\n",
    "\n",
    "Using _ResNet-18_ CNN trained from scratch on CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b97a03-8079-4ae6-8e6e-c2568fa2372b",
   "metadata": {
    "id": "67b97a03-8079-4ae6-8e6e-c2568fa2372b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy, pickle, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6e444-4aad-4810-a533-fd007980459e",
   "metadata": {
    "id": "27a6e444-4aad-4810-a533-fd007980459e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69060d84-943c-4b16-8090-75e76195f95f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69060d84-943c-4b16-8090-75e76195f95f",
    "outputId": "eebbe1a0-e3b5-4dbb-c380-e5cfc3286a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.8.1+cu101\n",
      "Torchvision Version: 0.9.1+cu101\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa0393-95cc-484d-a576-c6fcd608d5a0",
   "metadata": {
    "id": "b4aa0393-95cc-484d-a576-c6fcd608d5a0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448e1719-39f3-4a6e-a0b1-2c039fef7f36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "448e1719-39f3-4a6e-a0b1-2c039fef7f36",
    "outputId": "2b5bbbb7-0836-474d-b33f-1babe95b7655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently available device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration-\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"currently available device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696f03b-a4b2-456f-8cf7-04f502152354",
   "metadata": {
    "id": "0696f03b-a4b2-456f-8cf7-04f502152354"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b0ecb3d-aa51-434e-92df-a0c02eceae5c",
   "metadata": {
    "id": "9b0ecb3d-aa51-434e-92df-a0c02eceae5c"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters-\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba4c605c-06df-4422-9ae5-637fd954e738",
   "metadata": {
    "id": "ba4c605c-06df-4422-9ae5-637fd954e738"
   },
   "outputs": [],
   "source": [
    "# Define transformations for training and test sets-\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "      transforms.RandomCrop(32, padding = 4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "     ]\n",
    "     )\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "     ]\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e164694-7789-42c4-b367-3dbd33e83cac",
   "metadata": {
    "id": "7e164694-7789-42c4-b367-3dbd33e83cac"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Dataset has PILImage images of range [0, 1]. We transform them to Tensors\n",
    "# of normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ]\n",
    "        )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74106b6d-2dbf-4903-9ba9-ee28257b909a",
   "metadata": {
    "id": "74106b6d-2dbf-4903-9ba9-ee28257b909a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d343e6-aa64-4f3e-a177-00fff454c48a",
   "metadata": {
    "id": "45d343e6-aa64-4f3e-a177-00fff454c48a"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/arjun/Documents/Programs/Python_Codes/PyTorch_Resources/Good_Codes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676e2248-38a0-4a38-9e91-69b3538bdfe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "676e2248-38a0-4a38-9e91-69b3538bdfe4",
    "outputId": "8ac72c0f-4e4d-4848-8dbe-322a35dea7a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load dataset-\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root = './data', train = True,\n",
    "        download = True, transform = transform_train\n",
    "        )\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root = './data', train = False,\n",
    "        download = True, transform = transform_test\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8013f716-449a-4515-86aa-34dfd3323dbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8013f716-449a-4515-86aa-34dfd3323dbd",
    "outputId": "26c23461-f597-4818-9c8d-981bd83cbf98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset) = 50000 & len(test_dataset) = 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(train_dataset) = {len(train_dataset)} & len(test_dataset) = {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50fcf254-fcb7-4cd2-a0e7-45364689fb60",
   "metadata": {
    "id": "50fcf254-fcb7-4cd2-a0e7-45364689fb60"
   },
   "outputs": [],
   "source": [
    "# Create training and testing loaders-\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size = batch_size,\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size = batch_size,\n",
    "        shuffle = False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25d1d2a-aabf-4e95-a48a-beb5ec620acd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a25d1d2a-aabf-4e95-a48a-beb5ec620acd",
    "outputId": "c0f49a15-16e4-48f2-91a4-e4077ab159b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_loader) = 391 & len(test_loader) = 79\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(train_loader) = {len(train_loader)} & len(test_loader) = {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d3d64dd-2164-4559-8081-f5e8f0394c93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d3d64dd-2164-4559-8081-f5e8f0394c93",
    "outputId": "68f8e9ec-4cf9-4b90-e5e6-1ab3c0e0ff95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390.625, 78.125)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check-\n",
    "len(train_dataset) / batch_size, len(test_dataset) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d33867-6cc5-4fb9-8a48-ab94780ae7e0",
   "metadata": {
    "id": "24d33867-6cc5-4fb9-8a48-ab94780ae7e0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94478c85-78e5-41a6-9b8f-b7322e47c960",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94478c85-78e5-41a6-9b8f-b7322e47c960",
    "outputId": "190b15d2-874b-49f9-b7ce-08b6187405de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([128, 3, 32, 32]) & labels.shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Get some random training images-\n",
    "# some_img = iter(train_loader)\n",
    "# images, labels = some_img.next()\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# You get 32 images due to our specified batch size-\n",
    "print(f\"images.shape: {images.shape} & labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a653b-2851-48c5-a366-f1bc0ef29abd",
   "metadata": {
    "id": "ab4a653b-2851-48c5-a366-f1bc0ef29abd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b5fc1-f75b-4eda-b5ce-bd18efe2a18d",
   "metadata": {
    "id": "fd5b5fc1-f75b-4eda-b5ce-bd18efe2a18d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f191a7f-cf01-4f91-9f7c-6b468ae9f567",
   "metadata": {
    "id": "1f191a7f-cf01-4f91-9f7c-6b468ae9f567"
   },
   "source": [
    "### ResNet model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3833d224-adb0-47c7-9bbe-e44873f02c74",
   "metadata": {
    "id": "3833d224-adb0-47c7-9bbe-e44873f02c74"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1697e-7547-4624-b2a9-4c2248f4ee2b",
   "metadata": {
    "id": "3af1697e-7547-4624-b2a9-4c2248f4ee2b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c75c17db-dba5-428e-abcd-8c142f9609a9",
   "metadata": {
    "id": "c75c17db-dba5-428e-abcd-8c142f9609a9"
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d61ab-951c-4baf-8f36-7972ebc3ae17",
   "metadata": {
    "id": "830d61ab-951c-4baf-8f36-7972ebc3ae17"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7a6bc5d-800a-4033-a8ca-a48ee67c07f0",
   "metadata": {
    "id": "c7a6bc5d-800a-4033-a8ca-a48ee67c07f0"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c11b6b-a90f-40b2-8637-f0121aecc849",
   "metadata": {
    "id": "94c11b6b-a90f-40b2-8637-f0121aecc849"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96498f8-c04a-45f1-9f16-90f1cda7f213",
   "metadata": {
    "id": "e96498f8-c04a-45f1-9f16-90f1cda7f213"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff3126cf-d7fe-44d5-a7f4-695b25edcf30",
   "metadata": {
    "id": "ff3126cf-d7fe-44d5-a7f4-695b25edcf30"
   },
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6da023-6833-4424-aa76-12afe066a451",
   "metadata": {
    "id": "ca6da023-6833-4424-aa76-12afe066a451"
   },
   "outputs": [],
   "source": [
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3505e-16c3-4b2e-ae4e-f9ace2c8b07f",
   "metadata": {
    "id": "eba3505e-16c3-4b2e-ae4e-f9ace2c8b07f"
   },
   "outputs": [],
   "source": [
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47770fcb-01ca-4142-bf8d-705df50449ac",
   "metadata": {
    "id": "47770fcb-01ca-4142-bf8d-705df50449ac"
   },
   "outputs": [],
   "source": [
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a056f0-88a6-4393-beee-d62dc0f94ba7",
   "metadata": {
    "id": "92a056f0-88a6-4393-beee-d62dc0f94ba7"
   },
   "outputs": [],
   "source": [
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19c62a-e944-4457-9d05-cc596051e37f",
   "metadata": {
    "id": "5d19c62a-e944-4457-9d05-cc596051e37f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ca316e8-44b9-4f47-9d48-e879fe39f6e4",
   "metadata": {
    "id": "9ca316e8-44b9-4f47-9d48-e879fe39f6e4"
   },
   "outputs": [],
   "source": [
    "# Initialize ResNet-18 CNN model-\n",
    "trained_model = ResNet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "POeu-x4HVp9g",
   "metadata": {
    "id": "POeu-x4HVp9g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "MlKx60NXmD4K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "MlKx60NXmD4K",
    "outputId": "a3d09ba9-18dd-4dd5-ddf8-dca8c2988127"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rGByHeJAmFeb",
   "metadata": {
    "id": "rGByHeJAmFeb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "Awq4mZtNVa22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Awq4mZtNVa22",
    "outputId": "53edfa05-877c-4b30-cc48-0ac9702d299d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Move file from Google Colab to drive-\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0LuWMXvulu6h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0LuWMXvulu6h",
    "outputId": "5e3df5d4-4681-45d9-c737-583f01e96764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/content/drive/MyDrive/ResNet18_best_trained_loss.pth' -> '/content/ResNet18_best_trained_loss.pth'\n"
     ]
    }
   ],
   "source": [
    "# !cp -rv ResNet18_best_trained_loss.pth drive/MyDrive\n",
    "!cp -rv /content/drive/MyDrive/ResNet18_best_trained_loss.pth /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pcbvaUpLWK7_",
   "metadata": {
    "id": "pcbvaUpLWK7_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "S2N0v4mSmg9J",
   "metadata": {
    "id": "S2N0v4mSmg9J"
   },
   "outputs": [],
   "source": [
    "# Path to file-\n",
    "# PATH = \"/home/arjun/Deep_Learning_Resources/Computer_Vision_Resources/ResNet_resources/ResNet_Codes/Good_Codes/ResNet18-CosineAnnealing_LR_scheduler/ResNet18_best_trained_loss.pth\"\n",
    "PATH = \"ResNet18_best_trained_loss.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c3f01da-097d-4080-9f07-cda70404df8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c3f01da-097d-4080-9f07-cda70404df8e",
    "outputId": "70039349-90ad-4b99-d7de-849bd74fb88f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained weights-\n",
    "trained_model.load_state_dict(torch.load(PATH, map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae457a37-712d-4b53-9e6b-b360d89fe2e1",
   "metadata": {
    "id": "ae457a37-712d-4b53-9e6b-b360d89fe2e1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d49ee5d-9eac-4d52-92a0-7212f37dfb92",
   "metadata": {
    "id": "4d49ee5d-9eac-4d52-92a0-7212f37dfb92"
   },
   "outputs": [],
   "source": [
    "# Place model on GPU (if available)-\n",
    "trained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e97826-5845-48ca-b700-d3e0dce9895f",
   "metadata": {
    "id": "49e97826-5845-48ca-b700-d3e0dce9895f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d0801-eb58-4ac6-a95c-3d95ce4bb9ae",
   "metadata": {
    "id": "7a1d0801-eb58-4ac6-a95c-3d95ce4bb9ae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b732f13-47be-4b2a-98b7-a933a5f2db45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed3f70-c61d-4b64-b590-85e96609d05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7cc9fe7-2308-4c9b-8846-607860c5d6d1",
   "metadata": {},
   "source": [
    "## PyTorch Pruning:\n",
    "\n",
    "- [Reference blog](https://leimao.github.io/blog/PyTorch-Pruning/)\n",
    "\n",
    "- [Reference GitHub](https://github.com/leimao/PyTorch-Pruning-Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a463e869-22b4-4197-8c60-3cf3035dc2ff",
   "metadata": {},
   "source": [
    "### Sparsity for Iterative Pruning\n",
    "\n",
    "The ```prune.l1_unstructured``` function uses an ```amount``` argument which could be either the percentage of connections to prune (if it is a float between 0 and 1), or the absolute number of connections to prune (if it is a non-negative integer).\n",
    "\n",
    "__When it is the percentage, it is the the relative percentage to the number of unmasked/remaining parameters in the module/layer__.\n",
    "For example, in iterative pruning, if we prune the weights of a certain layer by ```amount = 0.2``` in the first iteration and then prune the same module/layer by the same ```amount = 0.2``` in the second iteration. Then:\n",
    "- _the amount of the valid/surviving parameters after the second round of pruning will be 1 x (1 - 0.2) x (1 - 0.2), (and)_\n",
    "- _the sparsity of the parameters, i.e., the pruning rate/rate of pruning, in this module/layer will be: 1 - (1 x (1 - 0.2) x (1 - 0.2))_.\n",
    "\n",
    "\n",
    "Formally, the final prune rate could be calculated using the following equation. Suppose that the relative pruning rate for each iteration is $\\gamma$, the final pruning rate, after _n_ iterations, will be:\n",
    "$1-\\left(1-\\gamma\\right)^n$\n",
    "\n",
    "\n",
    "Similarly, it is also easy to derive the final pruning rate for the scenario that is different in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c8e4c95-7fbc-42a1-9b48-36f7b077cb4c",
   "metadata": {
    "id": "5c8e4c95-7fbc-42a1-9b48-36f7b077cb4c"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, criterion = None):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels).item()\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    eval_loss = running_loss / len(test_loader.dataset)\n",
    "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
    "\n",
    "    return eval_loss, eval_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2igA9YNYcH_S",
   "metadata": {
    "id": "2igA9YNYcH_S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sBQyz92ycIK3",
   "metadata": {
    "id": "sBQyz92ycIK3"
   },
   "outputs": [],
   "source": [
    "def create_classification_report(model, device, test_loader):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            y_true += data[1].numpy().tolist()\n",
    "            images, _ = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_pred += predicted.cpu().numpy().tolist()\n",
    "\n",
    "    classification_report = sklearn.metrics.classification_report(\n",
    "        y_true = y_true, y_pred = y_pred)\n",
    "\n",
    "    return classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2WBDx3XkcLRY",
   "metadata": {
    "id": "2WBDx3XkcLRY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4u9B3ZultnI6",
   "metadata": {
    "id": "4u9B3ZultnI6"
   },
   "outputs": [],
   "source": [
    "def remove_parameters(model):\n",
    "\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            try:\n",
    "                prune.remove(module, \"weight\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                prune.remove(module, \"bias\")\n",
    "            except:\n",
    "                pass\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "            try:\n",
    "                prune.remove(module, \"weight\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                prune.remove(module, \"bias\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "alLzC6-Rtnii",
   "metadata": {
    "id": "alLzC6-Rtnii"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "NePRZMnWtn-3",
   "metadata": {
    "id": "NePRZMnWtn-3"
   },
   "outputs": [],
   "source": [
    "def compute_final_pruning_rate(pruning_rate, num_iterations):\n",
    "    '''\n",
    "    A function to compute the final pruning rate for iterative pruning.\n",
    "        Note that this cannot be applied for global pruning rate if the pruning rate is heterogeneous among different layers.\n",
    "    Args:\n",
    "        pruning_rate (float): Pruning rate.\n",
    "        num_iterations (int): Number of iterations.\n",
    "    Returns:\n",
    "        float: Final pruning rate.\n",
    "    '''\n",
    "\n",
    "    final_pruning_rate = 1 - (1 - pruning_rate) ** num_iterations\n",
    "\n",
    "    return final_pruning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08x_Gbn-tqgC",
   "metadata": {
    "id": "08x_Gbn-tqgC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "o5Q3K_kEtqoS",
   "metadata": {
    "id": "o5Q3K_kEtqoS"
   },
   "outputs": [],
   "source": [
    "def measure_module_sparsity(module, weight=True, bias=False, use_mask=False):\n",
    "\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "\n",
    "    if use_mask == True:\n",
    "        for buffer_name, buffer in module.named_buffers():\n",
    "            if \"weight_mask\" in buffer_name and weight == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "            if \"bias_mask\" in buffer_name and bias == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "    else:\n",
    "        for param_name, param in module.named_parameters():\n",
    "            if \"weight\" in param_name and weight == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "            if \"bias\" in param_name and bias == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "\n",
    "    sparsity = num_zeros / num_elements\n",
    "\n",
    "    return num_zeros, num_elements, sparsity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "Dg4iABhvtth4",
   "metadata": {
    "id": "Dg4iABhvtth4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "_Nt7szsPttoz",
   "metadata": {
    "id": "_Nt7szsPttoz"
   },
   "outputs": [],
   "source": [
    "def measure_global_sparsity(\n",
    "    model, weight = True,\n",
    "    bias = False, conv2d_use_mask = False,\n",
    "    linear_use_mask = False):\n",
    "\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "\n",
    "    for module_name, module in model.named_modules():\n",
    "\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
    "                module, weight=weight, bias=bias, use_mask=conv2d_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
    "                module, weight=weight, bias=bias, use_mask=linear_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "    sparsity = num_zeros / num_elements\n",
    "\n",
    "    return num_zeros, num_elements, sparsity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8Y5XR4JttwEh",
   "metadata": {
    "id": "8Y5XR4JttwEh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb65390-0bd9-458d-a27f-0e68d6ae63e5",
   "metadata": {
    "id": "1cb65390-0bd9-458d-a27f-0e68d6ae63e5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52bf2547-dd4a-45f3-95ec-b1cb5eb3e762",
   "metadata": {
    "id": "52bf2547-dd4a-45f3-95ec-b1cb5eb3e762"
   },
   "source": [
    "### torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "[refer](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "Decays the learning rate of each parameter group by 'gamma' once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When ```last_epoch = -1```, sets initial lr as lr.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- optimizer (Optimizer) – Wrapped optimizer.\n",
    "\n",
    "- milestones (list) – List of epoch indices. Must be increasing.\n",
    "\n",
    "- gamma (float) – Multiplicative factor of learning rate decay. Default: 0.1.\n",
    "\n",
    "- last_epoch (int) – The index of last epoch. Default: -1.\n",
    "\n",
    "- verbose (bool) – If True, prints a message to stdout for each update. Default: False.\n",
    "\n",
    "Example:\n",
    "```\n",
    "# Assuming optimizer uses lr = 0.05 for all groups\n",
    ">>> # lr = 0.05     if epoch < 30\n",
    ">>> # lr = 0.005    if 30 <= epoch < 80\n",
    ">>> # lr = 0.0005   if epoch >= 80\n",
    ">>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    ">>> for epoch in range(100):\n",
    ">>>     train(...)\n",
    ">>>     validate(...)\n",
    ">>>     scheduler.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06abeecd-82c7-4446-8acc-1f9f32a1903f",
   "metadata": {
    "id": "06abeecd-82c7-4446-8acc-1f9f32a1903f"
   },
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, test_loader, device, l1_regularization_strength = 0,\n",
    "# num_epochs = 200\n",
    "def fine_tune_train_model(model, train_loader, test_loader, device, l1_regularization_strength = 0,\n",
    "                l2_regularization_strength = 1e-4, learning_rate = 1e-1, num_epochs = 20):\n",
    "\n",
    "    # The training configurations were not carefully selected.\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10-\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), lr = learning_rate,\n",
    "        momentum = 0.9, weight_decay = l2_regularization_strength\n",
    "    )\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    # Define learning rate scheduler-\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        # optimizer, milestones = [100, 150],\n",
    "        optimizer, milestones = [5, 10],\n",
    "        gamma = 0.1, last_epoch = -1)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
    "    \n",
    "\n",
    "    # Evaluation-\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = evaluate_model(\n",
    "        model = model, test_loader = test_loader,\n",
    "        device = device, criterion = criterion)\n",
    "    \n",
    "    print(f\"Pre fine-tuning: val_loss = {eval_loss:.3f} & val_accuracy = {eval_accuracy * 100:.3f}%\")\n",
    "    # print(\"Epoch: {:03d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(0, eval_loss, eval_accuracy))\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            l1_reg = torch.tensor(0.).to(device)\n",
    "            for module in model.modules():\n",
    "                mask = None\n",
    "                weight = None\n",
    "                for name, buffer in module.named_buffers():\n",
    "                    if name == \"weight_mask\":\n",
    "                        mask = buffer\n",
    "                for name, param in module.named_parameters():\n",
    "                    if name == \"weight_orig\":\n",
    "                        weight = param\n",
    "                # We usually only want to introduce sparsity to weights and prune weights.\n",
    "                # Do the same for bias if necessary.\n",
    "                if mask is not None and weight is not None:\n",
    "                    l1_reg += torch.norm(mask * weight, 1)\n",
    "\n",
    "            loss += l1_regularization_strength * l1_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = evaluate_model(\n",
    "            model = model, test_loader = test_loader,\n",
    "            device = device, criterion = criterion)\n",
    "\n",
    "        # Set learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        '''\n",
    "        print(\n",
    "            \"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\"\n",
    "            .format(epoch + 1, train_loss, train_accuracy, eval_loss,\n",
    "                    eval_accuracy))\n",
    "        '''\n",
    "        print(f\"epoch = {epoch + 1} loss = {train_loss:.3f}, accuracy = {train_accuracy * 100:.3f}%, val_loss = {eval_loss:.3f}, val_accuracy = {eval_accuracy * 100:.3f}% & LR: {optimizer.param_groups[0]['lr']:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b167c5a7-0d58-4417-b76f-d0c8d42020b7",
   "metadata": {
    "id": "b167c5a7-0d58-4417-b76f-d0c8d42020b7"
   },
   "outputs": [],
   "source": [
    "# Sanity check-\n",
    "# fine_tuned_model = fine_tune_train_model(trained_model, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a7f35f-ad0f-40b2-8b8a-19a5913ac836",
   "metadata": {
    "id": "e7a7f35f-ad0f-40b2-8b8a-19a5913ac836"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e9f73-b3a3-4126-8a27-9ea825307a5b",
   "metadata": {
    "id": "b40e9f73-b3a3-4126-8a27-9ea825307a5b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cea484d-ee61-4065-acea-b0e3bd526de3",
   "metadata": {
    "id": "6cea484d-ee61-4065-acea-b0e3bd526de3"
   },
   "outputs": [],
   "source": [
    "def iterative_pruning_finetuning(\n",
    "    model, train_loader, test_loader, device,\n",
    "    learning_rate, l1_regularization_strength,\n",
    "    l2_regularization_strength, learning_rate_decay = 0.1,\n",
    "    conv2d_prune_amount = 0.2, linear_prune_amount = 0.1,\n",
    "    num_iterations = 10, num_epochs_per_iteration = 10,\n",
    "    model_filename_prefix = \"pruned_model\", model_dir = \"saved_models\",\n",
    "    grouped_pruning = False):\n",
    "    \n",
    "    '''\n",
    "    num_iterations - number of pruning iterations/rounds\n",
    "    num_epochs_per_iteration - number of fine-tuning rounds\n",
    "    '''\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        print(\"\\nPruning and Finetuning {}/{}\".format(i + 1, num_iterations))\n",
    "\n",
    "        print(\"Pruning...\")\n",
    "\n",
    "\n",
    "        # NOTE: For global pruning, linear/dense layer can also be pruned!\n",
    "        if grouped_pruning == True:\n",
    "            # grouped_pruning -> Global pruning\n",
    "            parameters_to_prune = []\n",
    "            for module_name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Conv2d):\n",
    "                    parameters_to_prune.append((module, \"weight\"))\n",
    "            prune.global_unstructured(\n",
    "                parameters_to_prune,\n",
    "                pruning_method = prune.L1Unstructured,\n",
    "                amount = conv2d_prune_amount,\n",
    "            )\n",
    "        else:\n",
    "            for module_name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Conv2d):\n",
    "                    prune.l1_unstructured(\n",
    "                        module, name = \"weight\",\n",
    "                        amount = conv2d_prune_amount)\n",
    "                elif isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(\n",
    "                        module, name = \"weight\",\n",
    "                        amount = linear_prune_amount)\n",
    "\n",
    "        # Compute validation accuracy just after pruning-\n",
    "        _, eval_accuracy = evaluate_model(\n",
    "            model = model, test_loader = test_loader,\n",
    "            device = device, criterion = None)\n",
    "\n",
    "        '''\n",
    "        classification_report = create_classification_report(\n",
    "            model=model, test_loader=test_loader, device=device)\n",
    "        '''\n",
    "\n",
    "        # Compute global sparsity-\n",
    "        num_zeros, num_elements, sparsity = measure_global_sparsity(\n",
    "            model, weight = True,\n",
    "            bias = False, conv2d_use_mask = True,\n",
    "            linear_use_mask = False)\n",
    "        \n",
    "        print(f\"Global sparsity = {sparsity * 100:.3f}% & val_accuracy = {eval_accuracy * 100:.3f}%\")\n",
    "        # print(model.conv1._forward_pre_hooks)\n",
    "\n",
    "        print(\"\\nFine-tuning...\")\n",
    "\n",
    "        # train_model(\n",
    "        fine_tuned_model = fine_tune_train_model(\n",
    "            model = model, train_loader = train_loader,\n",
    "            test_loader = test_loader, device = device,\n",
    "            l1_regularization_strength = l1_regularization_strength,\n",
    "            l2_regularization_strength = l2_regularization_strength,\n",
    "            # i -> current pruning round-\n",
    "            # learning_rate = learning_rate * (learning_rate_decay ** i),\n",
    "            learning_rate = 1e-1,\n",
    "            num_epochs = num_epochs_per_iteration)\n",
    "\n",
    "        _, eval_accuracy = evaluate_model(\n",
    "            model=model, test_loader = test_loader,\n",
    "            device = device, criterion = None)\n",
    "\n",
    "        '''\n",
    "        classification_report = create_classification_report(\n",
    "            model=model, test_loader=test_loader, device=device)\n",
    "        '''\n",
    "\n",
    "        num_zeros, num_elements, sparsity = measure_global_sparsity(\n",
    "            # model,\n",
    "            fine_tuned_model, weight = True,\n",
    "            bias = False, conv2d_use_mask = True,\n",
    "            linear_use_mask = False)\n",
    "\n",
    "        print(f\"Post fine-tuning: Global sparsity = {sparsity * 100:.3f}% & val_accuracy = {eval_accuracy * 100:.3f}%\")\n",
    "\n",
    "        '''\n",
    "        model_filename = \"{}_{}.pt\".format(model_filename_prefix, i + 1)\n",
    "        model_filepath = os.path.join(model_dir, model_filename)\n",
    "        save_model(model=model,\n",
    "                   model_dir=model_dir,\n",
    "                   model_filename=model_filename)\n",
    "        model = load_model(model=model,\n",
    "                           model_filepath=model_filepath,\n",
    "                           device=device)\n",
    "        '''\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55971543-d1d1-4f21-9ac0-f7cdcb53c13a",
   "metadata": {
    "id": "55971543-d1d1-4f21-9ac0-f7cdcb53c13a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf03de4-e2e8-403f-a67c-1fbe79df2830",
   "metadata": {
    "id": "0bf03de4-e2e8-403f-a67c-1fbe79df2830"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bf9a63f-03af-45e3-a893-b90d8aa059fe",
   "metadata": {
    "id": "4bf9a63f-03af-45e3-a893-b90d8aa059fe"
   },
   "outputs": [],
   "source": [
    "# num_classes = 10\n",
    "# random_seed = 1\n",
    "l1_regularization_strength = 0\n",
    "l2_regularization_strength = 1e-4\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17476676-4a45-4e17-9382-00080b4f5aa9",
   "metadata": {
    "id": "17476676-4a45-4e17-9382-00080b4f5aa9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af83a955-ea75-486f-8818-b5de1d569845",
   "metadata": {
    "id": "af83a955-ea75-486f-8818-b5de1d569845"
   },
   "outputs": [],
   "source": [
    "_, eval_accuracy = evaluate_model(\n",
    "    model = trained_model, test_loader=test_loader,\n",
    "    device = device, criterion = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "713d872a-970f-4cd5-8977-c08bbacac246",
   "metadata": {
    "id": "713d872a-970f-4cd5-8977-c08bbacac246"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "classification_report = create_classification_report(\n",
    "    model = trained_model, test_loader = test_loader,\n",
    "    device = device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2af261e3-beae-4715-a0bc-d43d643a271a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2af261e3-beae-4715-a0bc-d43d643a271a",
    "outputId": "0d700751-7120-49a9-ab2a-b87e87624364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity = 0.000% & val_accuracy = 88.990%\n"
     ]
    }
   ],
   "source": [
    "num_zeros, num_elements, sparsity = measure_global_sparsity(trained_model)\n",
    "print(f\"Global sparsity = {sparsity:.3f}% & val_accuracy = {eval_accuracy * 100:.3f}%\")\n",
    "\n",
    "# print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report)\n",
    "# print(\"Global Sparsity:\")\n",
    "# print(\"{:.2f}\".format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72664f76-d5ad-4529-ab07-fd4b737adc59",
   "metadata": {
    "id": "72664f76-d5ad-4529-ab07-fd4b737adc59"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6ef6922-a092-48ee-ab48-ba9fec2e442e",
   "metadata": {
    "id": "b6ef6922-a092-48ee-ab48-ba9fec2e442e"
   },
   "outputs": [],
   "source": [
    "model_dir = \"saved_models\"\n",
    "model_filename = \"resnet18_cifar10.pth\"\n",
    "model_filename_prefix = \"pruned_model\"\n",
    "pruned_model_filename = \"resnet18_pruned_cifar10.pth\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "pruned_model_filepath = os.path.join(model_dir, pruned_model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1TxqT9YXXQ52",
   "metadata": {
    "id": "1TxqT9YXXQ52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w7LP_7tIXRFS",
   "metadata": {
    "id": "w7LP_7tIXRFS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21531339-69ff-47cd-b480-0a04bf362b2c",
   "metadata": {
    "id": "21531339-69ff-47cd-b480-0a04bf362b2c"
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cc8bffa-4def-4174-9f67-9a0748209717",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cc8bffa-4def-4174-9f67-9a0748209717",
    "outputId": "940697a5-17e7-4ef0-a50d-7ada9846b406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative Pruning + Fine-Tuning...\n"
     ]
    }
   ],
   "source": [
    "print(\"Iterative Pruning + Fine-Tuning...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b4787ad-32ac-4083-bb48-fd19af860a1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b4787ad-32ac-4083-bb48-fd19af860a1e",
    "outputId": "4c95577b-7f90-47f4-909d-e290cb8b5ff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruning and Finetuning 1/15\n",
      "Pruning...\n",
      "Global sparsity = 19.991% & val_accuracy = 88.990%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.355 & val_accuracy = 88.990%\n",
      "epoch = 1 loss = 0.586, accuracy = 0.796, val_loss = 0.580, val_accuracy = 80.290 & LR: 0.1000\n",
      "epoch = 2 loss = 0.481, accuracy = 0.836, val_loss = 0.537, val_accuracy = 82.120 & LR: 0.1000\n",
      "epoch = 3 loss = 0.445, accuracy = 0.845, val_loss = 0.563, val_accuracy = 80.870 & LR: 0.1000\n",
      "epoch = 4 loss = 0.428, accuracy = 0.852, val_loss = 0.488, val_accuracy = 83.710 & LR: 0.1000\n",
      "epoch = 5 loss = 0.404, accuracy = 0.860, val_loss = 0.531, val_accuracy = 82.090 & LR: 0.0100\n",
      "epoch = 6 loss = 0.303, accuracy = 0.897, val_loss = 0.353, val_accuracy = 87.970 & LR: 0.0100\n",
      "epoch = 7 loss = 0.268, accuracy = 0.907, val_loss = 0.348, val_accuracy = 88.030 & LR: 0.0100\n",
      "epoch = 8 loss = 0.258, accuracy = 0.912, val_loss = 0.343, val_accuracy = 88.510 & LR: 0.0100\n",
      "epoch = 9 loss = 0.246, accuracy = 0.914, val_loss = 0.339, val_accuracy = 88.750 & LR: 0.0100\n",
      "epoch = 10 loss = 0.238, accuracy = 0.916, val_loss = 0.339, val_accuracy = 88.640 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 19.991% & val_accuracy = 88.640%\n",
      "\n",
      "Pruning and Finetuning 2/15\n",
      "Pruning...\n",
      "Global sparsity = 35.983% & val_accuracy = 88.640%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.339 & val_accuracy = 88.640%\n",
      "epoch = 1 loss = 0.389, accuracy = 0.864, val_loss = 0.483, val_accuracy = 83.880 & LR: 0.1000\n",
      "epoch = 2 loss = 0.383, accuracy = 0.867, val_loss = 0.459, val_accuracy = 84.540 & LR: 0.1000\n",
      "epoch = 3 loss = 0.367, accuracy = 0.873, val_loss = 0.438, val_accuracy = 85.200 & LR: 0.1000\n",
      "epoch = 4 loss = 0.362, accuracy = 0.875, val_loss = 0.429, val_accuracy = 85.420 & LR: 0.1000\n",
      "epoch = 5 loss = 0.352, accuracy = 0.878, val_loss = 0.468, val_accuracy = 84.160 & LR: 0.0100\n",
      "epoch = 6 loss = 0.260, accuracy = 0.910, val_loss = 0.339, val_accuracy = 88.530 & LR: 0.0100\n",
      "epoch = 7 loss = 0.228, accuracy = 0.920, val_loss = 0.330, val_accuracy = 89.170 & LR: 0.0100\n",
      "epoch = 8 loss = 0.217, accuracy = 0.926, val_loss = 0.325, val_accuracy = 89.410 & LR: 0.0100\n",
      "epoch = 9 loss = 0.210, accuracy = 0.927, val_loss = 0.331, val_accuracy = 89.410 & LR: 0.0100\n",
      "epoch = 10 loss = 0.202, accuracy = 0.928, val_loss = 0.330, val_accuracy = 89.360 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 35.983% & val_accuracy = 89.360%\n",
      "\n",
      "Pruning and Finetuning 3/15\n",
      "Pruning...\n",
      "Global sparsity = 48.778% & val_accuracy = 89.360%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.330 & val_accuracy = 89.360%\n",
      "epoch = 1 loss = 0.346, accuracy = 0.880, val_loss = 0.512, val_accuracy = 83.730 & LR: 0.1000\n",
      "epoch = 2 loss = 0.343, accuracy = 0.880, val_loss = 0.472, val_accuracy = 84.550 & LR: 0.1000\n",
      "epoch = 3 loss = 0.339, accuracy = 0.882, val_loss = 0.464, val_accuracy = 84.830 & LR: 0.1000\n",
      "epoch = 4 loss = 0.326, accuracy = 0.886, val_loss = 0.475, val_accuracy = 84.290 & LR: 0.1000\n",
      "epoch = 5 loss = 0.324, accuracy = 0.887, val_loss = 0.397, val_accuracy = 86.820 & LR: 0.0100\n",
      "epoch = 6 loss = 0.233, accuracy = 0.920, val_loss = 0.322, val_accuracy = 89.220 & LR: 0.0100\n",
      "epoch = 7 loss = 0.204, accuracy = 0.929, val_loss = 0.323, val_accuracy = 89.510 & LR: 0.0100\n",
      "epoch = 8 loss = 0.194, accuracy = 0.932, val_loss = 0.316, val_accuracy = 89.740 & LR: 0.0100\n",
      "epoch = 9 loss = 0.187, accuracy = 0.936, val_loss = 0.321, val_accuracy = 89.690 & LR: 0.0100\n",
      "epoch = 10 loss = 0.180, accuracy = 0.937, val_loss = 0.326, val_accuracy = 89.500 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 48.778% & val_accuracy = 89.500%\n",
      "\n",
      "Pruning and Finetuning 4/15\n",
      "Pruning...\n",
      "Global sparsity = 59.013% & val_accuracy = 89.500%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.326 & val_accuracy = 89.500%\n",
      "epoch = 1 loss = 0.317, accuracy = 0.890, val_loss = 0.420, val_accuracy = 86.070 & LR: 0.1000\n",
      "epoch = 2 loss = 0.318, accuracy = 0.888, val_loss = 0.454, val_accuracy = 84.970 & LR: 0.1000\n",
      "epoch = 3 loss = 0.312, accuracy = 0.890, val_loss = 0.440, val_accuracy = 86.170 & LR: 0.1000\n",
      "epoch = 4 loss = 0.309, accuracy = 0.893, val_loss = 0.405, val_accuracy = 86.450 & LR: 0.1000\n",
      "epoch = 5 loss = 0.304, accuracy = 0.894, val_loss = 0.425, val_accuracy = 86.190 & LR: 0.0100\n",
      "epoch = 6 loss = 0.220, accuracy = 0.925, val_loss = 0.322, val_accuracy = 89.380 & LR: 0.0100\n",
      "epoch = 7 loss = 0.193, accuracy = 0.933, val_loss = 0.318, val_accuracy = 89.620 & LR: 0.0100\n",
      "epoch = 8 loss = 0.175, accuracy = 0.939, val_loss = 0.315, val_accuracy = 89.820 & LR: 0.0100\n",
      "epoch = 9 loss = 0.170, accuracy = 0.941, val_loss = 0.319, val_accuracy = 89.900 & LR: 0.0100\n",
      "epoch = 10 loss = 0.164, accuracy = 0.943, val_loss = 0.319, val_accuracy = 89.960 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 59.013% & val_accuracy = 89.960%\n",
      "\n",
      "Pruning and Finetuning 5/15\n",
      "Pruning...\n",
      "Global sparsity = 67.201% & val_accuracy = 89.960%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.319 & val_accuracy = 89.960%\n",
      "epoch = 1 loss = 0.299, accuracy = 0.896, val_loss = 0.399, val_accuracy = 87.130 & LR: 0.1000\n",
      "epoch = 2 loss = 0.295, accuracy = 0.896, val_loss = 0.449, val_accuracy = 85.440 & LR: 0.1000\n",
      "epoch = 3 loss = 0.293, accuracy = 0.898, val_loss = 0.499, val_accuracy = 84.150 & LR: 0.1000\n",
      "epoch = 4 loss = 0.295, accuracy = 0.898, val_loss = 0.417, val_accuracy = 86.500 & LR: 0.1000\n",
      "epoch = 5 loss = 0.289, accuracy = 0.897, val_loss = 0.416, val_accuracy = 86.170 & LR: 0.0100\n",
      "epoch = 6 loss = 0.205, accuracy = 0.929, val_loss = 0.324, val_accuracy = 89.540 & LR: 0.0100\n",
      "epoch = 7 loss = 0.176, accuracy = 0.939, val_loss = 0.319, val_accuracy = 89.770 & LR: 0.0100\n",
      "epoch = 8 loss = 0.164, accuracy = 0.944, val_loss = 0.316, val_accuracy = 90.070 & LR: 0.0100\n",
      "epoch = 9 loss = 0.157, accuracy = 0.946, val_loss = 0.321, val_accuracy = 89.770 & LR: 0.0100\n",
      "epoch = 10 loss = 0.150, accuracy = 0.947, val_loss = 0.316, val_accuracy = 90.090 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 67.201% & val_accuracy = 90.090%\n",
      "\n",
      "Pruning and Finetuning 6/15\n",
      "Pruning...\n",
      "Global sparsity = 73.752% & val_accuracy = 90.090%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.316 & val_accuracy = 90.090%\n",
      "epoch = 1 loss = 0.285, accuracy = 0.900, val_loss = 0.575, val_accuracy = 81.540 & LR: 0.1000\n",
      "epoch = 2 loss = 0.286, accuracy = 0.900, val_loss = 0.429, val_accuracy = 86.270 & LR: 0.1000\n",
      "epoch = 3 loss = 0.283, accuracy = 0.901, val_loss = 0.531, val_accuracy = 83.480 & LR: 0.1000\n",
      "epoch = 4 loss = 0.286, accuracy = 0.899, val_loss = 0.392, val_accuracy = 86.990 & LR: 0.1000\n",
      "epoch = 5 loss = 0.278, accuracy = 0.902, val_loss = 0.426, val_accuracy = 86.590 & LR: 0.0100\n",
      "epoch = 6 loss = 0.194, accuracy = 0.932, val_loss = 0.315, val_accuracy = 89.910 & LR: 0.0100\n",
      "epoch = 7 loss = 0.164, accuracy = 0.944, val_loss = 0.305, val_accuracy = 90.310 & LR: 0.0100\n",
      "epoch = 8 loss = 0.154, accuracy = 0.947, val_loss = 0.309, val_accuracy = 90.560 & LR: 0.0100\n",
      "epoch = 9 loss = 0.143, accuracy = 0.950, val_loss = 0.311, val_accuracy = 90.380 & LR: 0.0100\n",
      "epoch = 10 loss = 0.138, accuracy = 0.952, val_loss = 0.313, val_accuracy = 90.510 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 73.752% & val_accuracy = 90.510%\n",
      "\n",
      "Pruning and Finetuning 7/15\n",
      "Pruning...\n",
      "Global sparsity = 78.992% & val_accuracy = 90.510%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.313 & val_accuracy = 90.510%\n",
      "epoch = 1 loss = 0.271, accuracy = 0.904, val_loss = 0.486, val_accuracy = 85.030 & LR: 0.1000\n",
      "epoch = 2 loss = 0.280, accuracy = 0.902, val_loss = 0.398, val_accuracy = 87.060 & LR: 0.1000\n",
      "epoch = 3 loss = 0.272, accuracy = 0.905, val_loss = 0.419, val_accuracy = 86.540 & LR: 0.1000\n",
      "epoch = 4 loss = 0.271, accuracy = 0.904, val_loss = 0.420, val_accuracy = 86.220 & LR: 0.1000\n",
      "epoch = 5 loss = 0.269, accuracy = 0.906, val_loss = 0.434, val_accuracy = 86.480 & LR: 0.0100\n",
      "epoch = 6 loss = 0.184, accuracy = 0.936, val_loss = 0.313, val_accuracy = 89.930 & LR: 0.0100\n",
      "epoch = 7 loss = 0.149, accuracy = 0.949, val_loss = 0.310, val_accuracy = 90.320 & LR: 0.0100\n",
      "epoch = 8 loss = 0.141, accuracy = 0.951, val_loss = 0.316, val_accuracy = 90.160 & LR: 0.0100\n",
      "epoch = 9 loss = 0.135, accuracy = 0.954, val_loss = 0.313, val_accuracy = 90.440 & LR: 0.0100\n",
      "epoch = 10 loss = 0.128, accuracy = 0.956, val_loss = 0.307, val_accuracy = 90.540 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 78.992% & val_accuracy = 90.540%\n",
      "\n",
      "Pruning and Finetuning 8/15\n",
      "Pruning...\n",
      "Global sparsity = 83.185% & val_accuracy = 90.540%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.307 & val_accuracy = 90.540%\n",
      "epoch = 1 loss = 0.265, accuracy = 0.908, val_loss = 0.430, val_accuracy = 86.370 & LR: 0.1000\n",
      "epoch = 2 loss = 0.266, accuracy = 0.907, val_loss = 0.431, val_accuracy = 86.610 & LR: 0.1000\n",
      "epoch = 3 loss = 0.265, accuracy = 0.907, val_loss = 0.468, val_accuracy = 85.600 & LR: 0.1000\n",
      "epoch = 4 loss = 0.263, accuracy = 0.907, val_loss = 0.420, val_accuracy = 86.850 & LR: 0.1000\n",
      "epoch = 5 loss = 0.262, accuracy = 0.907, val_loss = 0.424, val_accuracy = 87.010 & LR: 0.0100\n",
      "epoch = 6 loss = 0.173, accuracy = 0.941, val_loss = 0.302, val_accuracy = 90.490 & LR: 0.0100\n",
      "epoch = 7 loss = 0.145, accuracy = 0.949, val_loss = 0.300, val_accuracy = 90.610 & LR: 0.0100\n",
      "epoch = 8 loss = 0.133, accuracy = 0.954, val_loss = 0.302, val_accuracy = 90.570 & LR: 0.0100\n",
      "epoch = 9 loss = 0.126, accuracy = 0.957, val_loss = 0.304, val_accuracy = 90.640 & LR: 0.0100\n",
      "epoch = 10 loss = 0.120, accuracy = 0.959, val_loss = 0.303, val_accuracy = 90.860 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 83.185% & val_accuracy = 90.860%\n",
      "\n",
      "Pruning and Finetuning 9/15\n",
      "Pruning...\n",
      "Global sparsity = 86.539% & val_accuracy = 90.860%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.303 & val_accuracy = 90.860%\n",
      "epoch = 1 loss = 0.259, accuracy = 0.907, val_loss = 0.453, val_accuracy = 85.900 & LR: 0.1000\n",
      "epoch = 2 loss = 0.258, accuracy = 0.909, val_loss = 0.413, val_accuracy = 87.120 & LR: 0.1000\n",
      "epoch = 3 loss = 0.255, accuracy = 0.911, val_loss = 0.419, val_accuracy = 86.840 & LR: 0.1000\n",
      "epoch = 4 loss = 0.255, accuracy = 0.910, val_loss = 0.392, val_accuracy = 87.240 & LR: 0.1000\n",
      "epoch = 5 loss = 0.245, accuracy = 0.915, val_loss = 0.426, val_accuracy = 86.410 & LR: 0.0100\n",
      "epoch = 6 loss = 0.163, accuracy = 0.944, val_loss = 0.301, val_accuracy = 90.100 & LR: 0.0100\n",
      "epoch = 7 loss = 0.134, accuracy = 0.954, val_loss = 0.304, val_accuracy = 90.250 & LR: 0.0100\n",
      "epoch = 8 loss = 0.125, accuracy = 0.957, val_loss = 0.302, val_accuracy = 90.700 & LR: 0.0100\n",
      "epoch = 9 loss = 0.117, accuracy = 0.960, val_loss = 0.304, val_accuracy = 90.620 & LR: 0.0100\n",
      "epoch = 10 loss = 0.112, accuracy = 0.961, val_loss = 0.303, val_accuracy = 90.740 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 86.539% & val_accuracy = 90.740%\n",
      "\n",
      "Pruning and Finetuning 10/15\n",
      "Pruning...\n",
      "Global sparsity = 89.222% & val_accuracy = 90.740%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.303 & val_accuracy = 90.740%\n",
      "epoch = 1 loss = 0.249, accuracy = 0.913, val_loss = 0.418, val_accuracy = 86.710 & LR: 0.1000\n",
      "epoch = 2 loss = 0.250, accuracy = 0.912, val_loss = 0.413, val_accuracy = 86.960 & LR: 0.1000\n",
      "epoch = 3 loss = 0.248, accuracy = 0.913, val_loss = 0.445, val_accuracy = 85.370 & LR: 0.1000\n",
      "epoch = 4 loss = 0.245, accuracy = 0.915, val_loss = 0.456, val_accuracy = 85.910 & LR: 0.1000\n",
      "epoch = 5 loss = 0.246, accuracy = 0.913, val_loss = 0.427, val_accuracy = 86.860 & LR: 0.0100\n",
      "epoch = 6 loss = 0.159, accuracy = 0.945, val_loss = 0.296, val_accuracy = 90.490 & LR: 0.0100\n",
      "epoch = 7 loss = 0.128, accuracy = 0.957, val_loss = 0.293, val_accuracy = 90.640 & LR: 0.0100\n",
      "epoch = 8 loss = 0.119, accuracy = 0.959, val_loss = 0.297, val_accuracy = 90.760 & LR: 0.0100\n",
      "epoch = 9 loss = 0.110, accuracy = 0.962, val_loss = 0.295, val_accuracy = 90.800 & LR: 0.0100\n",
      "epoch = 10 loss = 0.104, accuracy = 0.964, val_loss = 0.303, val_accuracy = 91.000 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 89.222% & val_accuracy = 91.000%\n",
      "\n",
      "Pruning and Finetuning 11/15\n",
      "Pruning...\n",
      "Global sparsity = 91.368% & val_accuracy = 91.000%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.303 & val_accuracy = 91.000%\n",
      "epoch = 1 loss = 0.246, accuracy = 0.913, val_loss = 0.473, val_accuracy = 85.730 & LR: 0.1000\n",
      "epoch = 2 loss = 0.242, accuracy = 0.915, val_loss = 0.432, val_accuracy = 86.380 & LR: 0.1000\n",
      "epoch = 3 loss = 0.240, accuracy = 0.917, val_loss = 0.392, val_accuracy = 87.800 & LR: 0.1000\n",
      "epoch = 4 loss = 0.238, accuracy = 0.915, val_loss = 0.416, val_accuracy = 86.830 & LR: 0.1000\n",
      "epoch = 5 loss = 0.240, accuracy = 0.914, val_loss = 0.427, val_accuracy = 86.630 & LR: 0.0100\n",
      "epoch = 6 loss = 0.153, accuracy = 0.948, val_loss = 0.292, val_accuracy = 90.710 & LR: 0.0100\n",
      "epoch = 7 loss = 0.122, accuracy = 0.959, val_loss = 0.290, val_accuracy = 90.830 & LR: 0.0100\n",
      "epoch = 8 loss = 0.112, accuracy = 0.961, val_loss = 0.293, val_accuracy = 90.760 & LR: 0.0100\n",
      "epoch = 9 loss = 0.104, accuracy = 0.964, val_loss = 0.295, val_accuracy = 91.120 & LR: 0.0100\n",
      "epoch = 10 loss = 0.101, accuracy = 0.965, val_loss = 0.292, val_accuracy = 91.180 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 91.368% & val_accuracy = 91.180%\n",
      "\n",
      "Pruning and Finetuning 12/15\n",
      "Pruning...\n",
      "Global sparsity = 93.085% & val_accuracy = 91.180%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.292 & val_accuracy = 91.180%\n",
      "epoch = 1 loss = 0.231, accuracy = 0.919, val_loss = 0.412, val_accuracy = 87.210 & LR: 0.1000\n",
      "epoch = 2 loss = 0.230, accuracy = 0.919, val_loss = 0.428, val_accuracy = 86.820 & LR: 0.1000\n",
      "epoch = 3 loss = 0.233, accuracy = 0.918, val_loss = 0.418, val_accuracy = 87.240 & LR: 0.1000\n",
      "epoch = 4 loss = 0.231, accuracy = 0.917, val_loss = 0.428, val_accuracy = 86.930 & LR: 0.1000\n",
      "epoch = 5 loss = 0.229, accuracy = 0.918, val_loss = 0.397, val_accuracy = 86.940 & LR: 0.0100\n",
      "epoch = 6 loss = 0.152, accuracy = 0.948, val_loss = 0.295, val_accuracy = 90.500 & LR: 0.0100\n",
      "epoch = 7 loss = 0.117, accuracy = 0.959, val_loss = 0.289, val_accuracy = 90.900 & LR: 0.0100\n",
      "epoch = 8 loss = 0.108, accuracy = 0.963, val_loss = 0.293, val_accuracy = 90.760 & LR: 0.0100\n",
      "epoch = 9 loss = 0.100, accuracy = 0.966, val_loss = 0.289, val_accuracy = 90.970 & LR: 0.0100\n",
      "epoch = 10 loss = 0.092, accuracy = 0.968, val_loss = 0.294, val_accuracy = 91.020 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 93.085% & val_accuracy = 91.020%\n",
      "\n",
      "Pruning and Finetuning 13/15\n",
      "Pruning...\n",
      "Global sparsity = 94.459% & val_accuracy = 91.000%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.294 & val_accuracy = 91.000%\n",
      "epoch = 1 loss = 0.220, accuracy = 0.923, val_loss = 0.476, val_accuracy = 85.460 & LR: 0.1000\n",
      "epoch = 2 loss = 0.225, accuracy = 0.920, val_loss = 0.412, val_accuracy = 87.360 & LR: 0.1000\n",
      "epoch = 3 loss = 0.226, accuracy = 0.922, val_loss = 0.410, val_accuracy = 86.920 & LR: 0.1000\n",
      "epoch = 4 loss = 0.220, accuracy = 0.923, val_loss = 0.434, val_accuracy = 86.230 & LR: 0.1000\n",
      "epoch = 5 loss = 0.225, accuracy = 0.919, val_loss = 0.441, val_accuracy = 86.330 & LR: 0.0100\n",
      "epoch = 6 loss = 0.140, accuracy = 0.952, val_loss = 0.293, val_accuracy = 90.510 & LR: 0.0100\n",
      "epoch = 7 loss = 0.111, accuracy = 0.963, val_loss = 0.297, val_accuracy = 90.760 & LR: 0.0100\n",
      "epoch = 8 loss = 0.099, accuracy = 0.966, val_loss = 0.298, val_accuracy = 90.820 & LR: 0.0100\n",
      "epoch = 9 loss = 0.093, accuracy = 0.968, val_loss = 0.302, val_accuracy = 90.700 & LR: 0.0100\n",
      "epoch = 10 loss = 0.083, accuracy = 0.972, val_loss = 0.305, val_accuracy = 90.800 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 94.459% & val_accuracy = 90.800%\n",
      "\n",
      "Pruning and Finetuning 14/15\n",
      "Pruning...\n",
      "Global sparsity = 95.558% & val_accuracy = 90.830%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.305 & val_accuracy = 90.830%\n",
      "epoch = 1 loss = 0.209, accuracy = 0.927, val_loss = 0.461, val_accuracy = 85.730 & LR: 0.1000\n",
      "epoch = 2 loss = 0.213, accuracy = 0.924, val_loss = 0.454, val_accuracy = 85.750 & LR: 0.1000\n",
      "epoch = 3 loss = 0.212, accuracy = 0.925, val_loss = 0.441, val_accuracy = 86.860 & LR: 0.1000\n",
      "epoch = 4 loss = 0.213, accuracy = 0.925, val_loss = 0.421, val_accuracy = 86.880 & LR: 0.1000\n",
      "epoch = 5 loss = 0.208, accuracy = 0.927, val_loss = 0.447, val_accuracy = 86.970 & LR: 0.0100\n",
      "epoch = 6 loss = 0.133, accuracy = 0.954, val_loss = 0.294, val_accuracy = 90.940 & LR: 0.0100\n",
      "epoch = 7 loss = 0.103, accuracy = 0.966, val_loss = 0.297, val_accuracy = 91.240 & LR: 0.0100\n",
      "epoch = 8 loss = 0.094, accuracy = 0.968, val_loss = 0.295, val_accuracy = 91.410 & LR: 0.0100\n",
      "epoch = 9 loss = 0.084, accuracy = 0.971, val_loss = 0.300, val_accuracy = 91.530 & LR: 0.0100\n",
      "epoch = 10 loss = 0.080, accuracy = 0.972, val_loss = 0.305, val_accuracy = 91.700 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 95.558% & val_accuracy = 91.700%\n",
      "\n",
      "Pruning and Finetuning 15/15\n",
      "Pruning...\n",
      "Global sparsity = 96.437% & val_accuracy = 91.630%\n",
      "\n",
      "Fine-tuning...\n",
      "Pre fine-tuning: val_loss = 0.305 & val_accuracy = 91.630%\n",
      "epoch = 1 loss = 0.194, accuracy = 0.932, val_loss = 0.410, val_accuracy = 87.440 & LR: 0.1000\n",
      "epoch = 2 loss = 0.195, accuracy = 0.930, val_loss = 0.514, val_accuracy = 84.550 & LR: 0.1000\n",
      "epoch = 3 loss = 0.197, accuracy = 0.930, val_loss = 0.454, val_accuracy = 86.870 & LR: 0.1000\n",
      "epoch = 4 loss = 0.196, accuracy = 0.931, val_loss = 0.427, val_accuracy = 87.500 & LR: 0.1000\n",
      "epoch = 5 loss = 0.195, accuracy = 0.930, val_loss = 0.365, val_accuracy = 88.710 & LR: 0.0100\n",
      "epoch = 6 loss = 0.120, accuracy = 0.959, val_loss = 0.293, val_accuracy = 90.950 & LR: 0.0100\n",
      "epoch = 7 loss = 0.092, accuracy = 0.969, val_loss = 0.291, val_accuracy = 91.080 & LR: 0.0100\n",
      "epoch = 8 loss = 0.082, accuracy = 0.972, val_loss = 0.293, val_accuracy = 91.260 & LR: 0.0100\n",
      "epoch = 9 loss = 0.075, accuracy = 0.974, val_loss = 0.299, val_accuracy = 91.190 & LR: 0.0100\n",
      "epoch = 10 loss = 0.071, accuracy = 0.976, val_loss = 0.302, val_accuracy = 91.320 & LR: 0.0010\n",
      "Post fine-tuning: Global sparsity = 96.437% & val_accuracy = 91.320%\n",
      "Global sparsity = 0.964 & val_accuracy = 0.913\n"
     ]
    }
   ],
   "source": [
    "# Create a deep copy of the pre-trained model-\n",
    "pruned_model = copy.deepcopy(trained_model)\n",
    "\n",
    "\n",
    "# Prune and fine-tune trained model-\n",
    "'''\n",
    "num_iterations - number of pruning iterations/rounds\n",
    "num_epochs_per_iteration - number of fine-tuning rounds\n",
    "'''\n",
    "pruned_model = iterative_pruning_finetuning(\n",
    "        model = pruned_model, train_loader = train_loader,\n",
    "        test_loader = test_loader, device = device,\n",
    "        learning_rate = learning_rate, learning_rate_decay = learning_rate_decay,\n",
    "        l1_regularization_strength = l1_regularization_strength, l2_regularization_strength = l2_regularization_strength,\n",
    "        conv2d_prune_amount = 0.2, linear_prune_amount = 0.1,\n",
    "        num_iterations = 15, num_epochs_per_iteration = 10,\n",
    "        model_filename_prefix = model_filename_prefix, model_dir = model_dir,\n",
    "        grouped_pruning = True)\n",
    "\n",
    "\n",
    "# Apply pruned mask to the parameters/weights and remove the masks-\n",
    "remove_parameters(model = pruned_model)\n",
    "\n",
    "_, eval_accuracy = evaluate_model(\n",
    "    model = pruned_model, test_loader = test_loader,\n",
    "    device = device, criterion = None\n",
    ")\n",
    "\n",
    "'''\n",
    "classification_report = create_classification_report(\n",
    "    model = pruned_model, test_loader = test_loader,\n",
    "    device = device)\n",
    "'''\n",
    "\n",
    "num_zeros, num_elements, sparsity = measure_global_sparsity(pruned_model)\n",
    "\n",
    "\n",
    "print(f\"Global sparsity = {sparsity:.3f} & val_accuracy = {eval_accuracy:.3f}\")\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report)\n",
    "# NOTE: classification report is avoided as it's too verbose!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZAw22aywnj-n",
   "metadata": {
    "id": "ZAw22aywnj-n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "wKkt-PB1SXRW",
   "metadata": {
    "id": "wKkt-PB1SXRW"
   },
   "outputs": [],
   "source": [
    "# Remove pruning parameters-\n",
    "final_model = remove_parameters(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a24a695-dc89-470e-8d85-5453ecc82deb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a24a695-dc89-470e-8d85-5453ecc82deb",
    "outputId": "705d6d7d-893d-485d-cedb-56baf2285671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity = 96.437% & val_accuracy = 91.320%\n"
     ]
    }
   ],
   "source": [
    "# Compute final model's val_accuracy and global sparsity-\n",
    "_, eval_accuracy = evaluate_model(\n",
    "    model = final_model, test_loader = test_loader,\n",
    "    device = device, criterion = None)\n",
    "\n",
    "num_zeros, num_elements, sparsity = measure_global_sparsity(pruned_model)\n",
    "print(f\"Global sparsity = {sparsity * 100:.3f}%\"\n",
    "f\" & val_accuracy = {eval_accuracy * 100:.3f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "niTw6STRPFRV",
   "metadata": {
    "id": "niTw6STRPFRV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "RvI-uCRJPKBe",
   "metadata": {
    "id": "RvI-uCRJPKBe"
   },
   "outputs": [],
   "source": [
    "# Save final trained and pruned model for later use-\n",
    "torch.save(final_model.state_dict(), f\"ResNet18_trained_sparsity-{sparsity * 100:.3f}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7CVnjpFbPq06",
   "metadata": {
    "id": "7CVnjpFbPq06"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AKB2rtbDREL-",
   "metadata": {
    "id": "AKB2rtbDREL-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0etCF3nGRE0x",
   "metadata": {
    "id": "0etCF3nGRE0x"
   },
   "source": [
    "### Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "FlGMUlEtPq6I",
   "metadata": {
    "id": "FlGMUlEtPq6I"
   },
   "outputs": [],
   "source": [
    "# Initialize and load trained and pruned model-\n",
    "trained_pruned_model = ResNet18()\n",
    "trained_pruned_model.load_state_dict(torch.load(\"/content/ResNet18_trained_sparsity-96.437.pth\", map_location = device))\n",
    "# trained_pruned_model.load_state_dict(final_model)\n",
    "\n",
    "# Move model to GPU (if available)-\n",
    "trained_pruned_model.to(device)\n",
    "\n",
    "# Define cost function and optimizer-\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10-\n",
    "optimizer = torch.optim.SGD(\n",
    "        trained_pruned_model.parameters(), lr = learning_rate,\n",
    "        momentum = 0.9, weight_decay = l2_regularization_strength\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "yKyL1Z1AQOlq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKyL1Z1AQOlq",
    "outputId": "93a7f07a-e2bc-4094-fbe9-bec29391f707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity = 96.437%, val_loss = 0.000 & val_accuracy = 91.320%\n"
     ]
    }
   ],
   "source": [
    "# Compute final model's val_accuracy and global sparsity-\n",
    "eval_loss, eval_accuracy = evaluate_model(\n",
    "    model = trained_pruned_model, test_loader=test_loader,\n",
    "    device = device, criterion = None)\n",
    "\n",
    "num_zeros, num_elements, sparsity = measure_global_sparsity(trained_pruned_model)\n",
    "print(f\"Global sparsity = {sparsity * 100:.3f}%, val_loss = {eval_loss:.3f}\"\n",
    "f\" & val_accuracy = {eval_accuracy * 100:.3f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39tGyip0QgdY",
   "metadata": {
    "id": "39tGyip0QgdY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CPfhqN7EQggV",
   "metadata": {
    "id": "CPfhqN7EQggV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet18_Global_Pruning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
