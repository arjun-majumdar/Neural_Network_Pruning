{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ResNet18_Global_Magnitude_Custom_Pruning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "097e037269c04184b0031b2e741de36d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2194d3901bb34d38b162dbfc9e18b544",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f6fb82d2bdc343f4b7279b7837cc1e8b",
              "IPY_MODEL_0888582886b3409291aa88a11dbad8e8"
            ]
          }
        },
        "2194d3901bb34d38b162dbfc9e18b544": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6fb82d2bdc343f4b7279b7837cc1e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_05813ac6f1e848b49196eb4657e29ad3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f9317d5d0264fc0bb6eb4a4544c5432"
          }
        },
        "0888582886b3409291aa88a11dbad8e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_46c33e095eab4cbfb0e8729910a53861",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [02:03&lt;00:00, 1382785.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fcaf24a723794756abf8ede1c3e7ef54"
          }
        },
        "05813ac6f1e848b49196eb4657e29ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f9317d5d0264fc0bb6eb4a4544c5432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46c33e095eab4cbfb0e8729910a53861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fcaf24a723794756abf8ede1c3e7ef54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prescription-anthony"
      },
      "source": [
        "# PyTorch Global, Unstructured, Absolute Magnitude & Iterative Pruning:\n",
        "\n",
        "Using _ResNet-18_ CNN trained from scratch on CIFAR-10 dataset."
      ],
      "id": "prescription-anthony"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "photographic-andrews"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import copy, pickle, os"
      ],
      "id": "photographic-andrews",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "formal-vatican"
      },
      "source": [
        ""
      ],
      "id": "formal-vatican",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bronze-ghost",
        "outputId": "490113ba-aef6-41f5-92eb-fd6947a2d675"
      },
      "source": [
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"Torchvision Version: {torchvision.__version__}\")"
      ],
      "id": "bronze-ghost",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Version: 1.8.1+cu101\n",
            "Torchvision Version: 0.9.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foreign-cotton"
      },
      "source": [
        ""
      ],
      "id": "foreign-cotton",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prostate-publisher",
        "outputId": "250bbc16-9844-4275-bc7e-787fddd1e5a6"
      },
      "source": [
        "# Device configuration-\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"currently available device: {device}\")"
      ],
      "id": "prostate-publisher",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "currently available device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "common-avatar",
        "outputId": "304a0702-00c0-4526-aa1f-a88c6cfb38be"
      },
      "source": [
        "%env CUDA_DEVICE_ORDER = PCI_BUS_ID\n",
        "%env CUDA_VISIBLE_DEVICES = 2"
      ],
      "id": "common-avatar",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
            "env: CUDA_VISIBLE_DEVICES=2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "durable-confusion"
      },
      "source": [
        ""
      ],
      "id": "durable-confusion",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "outside-tampa"
      },
      "source": [
        "# Hyper-parameters-\n",
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "learning_rate = 0.1"
      ],
      "id": "outside-tampa",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unlikely-potter"
      },
      "source": [
        "# Define transformations for training and test sets-\n",
        "transform_train = transforms.Compose(\n",
        "    [\n",
        "      transforms.RandomCrop(32, padding = 4),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "     ]\n",
        "     )\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "    [\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "     ]\n",
        "     )"
      ],
      "id": "unlikely-potter",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sought-score"
      },
      "source": [
        "# os.chdir(\"/home/arjun/Documents/Programs/Python_Codes/PyTorch_Resources/Good_Codes/\")"
      ],
      "id": "sought-score",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "097e037269c04184b0031b2e741de36d",
            "2194d3901bb34d38b162dbfc9e18b544",
            "f6fb82d2bdc343f4b7279b7837cc1e8b",
            "0888582886b3409291aa88a11dbad8e8",
            "05813ac6f1e848b49196eb4657e29ad3",
            "3f9317d5d0264fc0bb6eb4a4544c5432",
            "46c33e095eab4cbfb0e8729910a53861",
            "fcaf24a723794756abf8ede1c3e7ef54"
          ]
        },
        "id": "surprised-milton",
        "outputId": "64088438-42c2-493a-9142-c3285577cb89"
      },
      "source": [
        "# Load dataset-\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root = './data', train = True,\n",
        "        download = True, transform = transform_train\n",
        "        )\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root = './data', train = False,\n",
        "        download = True, transform = transform_test\n",
        "        )"
      ],
      "id": "surprised-milton",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "097e037269c04184b0031b2e741de36d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "another-redhead",
        "outputId": "00edf6d7-1500-42b4-aae5-352408914def"
      },
      "source": [
        "print(f\"len(train_dataset) = {len(train_dataset)} & len(test_dataset) = {len(test_dataset)}\")"
      ],
      "id": "another-redhead",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(train_dataset) = 50000 & len(test_dataset) = 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knowing-defendant"
      },
      "source": [
        "# Create training and testing loaders-\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size = batch_size,\n",
        "        shuffle = True\n",
        "        )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size = batch_size,\n",
        "        shuffle = False\n",
        "        )"
      ],
      "id": "knowing-defendant",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "developing-token",
        "outputId": "b4acd0ed-224c-4459-e8b1-e54f5ec34e56"
      },
      "source": [
        "print(f\"len(train_loader) = {len(train_loader)} & len(test_loader) = {len(test_loader)}\")"
      ],
      "id": "developing-token",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(train_loader) = 391 & len(test_loader) = 79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pharmaceutical-minimum",
        "outputId": "34ddeae7-8c06-4568-e85a-d34dab582be6"
      },
      "source": [
        "# Sanity check-\n",
        "len(train_dataset) / batch_size, len(test_dataset) / batch_size"
      ],
      "id": "pharmaceutical-minimum",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(390.625, 78.125)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "military-headquarters"
      },
      "source": [
        ""
      ],
      "id": "military-headquarters",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "digital-track",
        "outputId": "190b15d2-874b-49f9-b7ce-08b6187405de"
      },
      "source": [
        "# Get some random training images-\n",
        "# some_img = iter(train_loader)\n",
        "# images, labels = some_img.next()\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "# You get 32 images due to our specified batch size-\n",
        "print(f\"images.shape: {images.shape} & labels.shape: {labels.shape}\")"
      ],
      "id": "digital-track",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images.shape: torch.Size([128, 3, 32, 32]) & labels.shape: torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accessible-orchestra"
      },
      "source": [
        ""
      ],
      "id": "accessible-orchestra",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heard-likelihood"
      },
      "source": [
        ""
      ],
      "id": "heard-likelihood",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "surprising-appreciation"
      },
      "source": [
        "### ResNet model definition:"
      ],
      "id": "surprising-appreciation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elder-instruction"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n"
      ],
      "id": "elder-instruction",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "appointed-vacuum"
      },
      "source": [
        ""
      ],
      "id": "appointed-vacuum",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gorgeous-ballot"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n"
      ],
      "id": "gorgeous-ballot",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "satisfactory-retailer"
      },
      "source": [
        ""
      ],
      "id": "satisfactory-retailer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fewer-stream"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ],
      "id": "fewer-stream",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "respective-nursing"
      },
      "source": [
        ""
      ],
      "id": "respective-nursing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geological-climb"
      },
      "source": [
        ""
      ],
      "id": "geological-climb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comprehensive-powell"
      },
      "source": [
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])"
      ],
      "id": "comprehensive-powell",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sporting-lyric"
      },
      "source": [
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])"
      ],
      "id": "sporting-lyric",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "economic-cooling"
      },
      "source": [
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])"
      ],
      "id": "economic-cooling",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alpine-minister"
      },
      "source": [
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])"
      ],
      "id": "alpine-minister",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imported-combat"
      },
      "source": [
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])"
      ],
      "id": "imported-combat",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "resistant-immigration"
      },
      "source": [
        ""
      ],
      "id": "resistant-immigration",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "respective-bruce"
      },
      "source": [
        ""
      ],
      "id": "respective-bruce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "renewable-watson"
      },
      "source": [
        "import gc"
      ],
      "id": "renewable-watson",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cooked-blind"
      },
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "id": "cooked-blind",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "novel-effects"
      },
      "source": [
        "foo = torch.tensor([1,2,3])\n",
        "foo = foo.to('cuda')"
      ],
      "id": "novel-effects",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "logical-chemistry"
      },
      "source": [
        ""
      ],
      "id": "logical-chemistry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brief-classification"
      },
      "source": [
        ""
      ],
      "id": "brief-classification",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phantom-attempt"
      },
      "source": [
        "# Initialize ResNet-18 CNN model-\n",
        "trained_model = ResNet18()"
      ],
      "id": "phantom-attempt",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "preceding-relationship"
      },
      "source": [
        ""
      ],
      "id": "preceding-relationship",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "capital-istanbul",
        "outputId": "d10d9d14-e025-49c4-9ab0-0aa97fc9752e"
      },
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "id": "capital-istanbul",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "central-citation",
        "outputId": "44cbe68e-3bf2-47cb-d926-6a518dc221c4"
      },
      "source": [
        "\n",
        "# Move file from Google Colab to drive-\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "id": "central-citation",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faced-eating",
        "outputId": "5c2612fa-55c0-46e8-f4af-ca47df9e107f"
      },
      "source": [
        "# !cp -rv ResNet18_best_trained_loss.pth drive/MyDrive\n",
        "!cp -rv /content/drive/MyDrive/ResNet18_best_trained_loss.pth /content"
      ],
      "id": "faced-eating",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/MyDrive/ResNet18_best_trained_loss.pth' -> '/content/ResNet18_best_trained_loss.pth'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YBzOdHZ3ApXK",
        "outputId": "c6b5c562-f192-4e83-d635-ef2c8d01a761"
      },
      "source": [
        "os.getcwd()"
      ],
      "id": "YBzOdHZ3ApXK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANQlo6ovArQp",
        "outputId": "906f4c18-2440-4b25-d964-9c6a873ff0b7"
      },
      "source": [
        "os.listdir()"
      ],
      "id": "ANQlo6ovArQp",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'data', 'ResNet18_best_trained_loss.pth', 'drive', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "known-dakota"
      },
      "source": [
        "# Path to file-\n",
        "# PATH = \"/home/arjun/Deep_Learning_Resources/Computer_Vision_Resources/ResNet_resources/ResNet_Codes/Good_Codes/ResNet18-CosineAnnealing_LR_scheduler/ResNet18_best_trained_loss.pth\"\n",
        "# PATH = \"Weights/\"\n",
        "PATH = \"/content/\""
      ],
      "id": "known-dakota",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meaning-correlation",
        "outputId": "7e0da567-4d88-4210-955c-08c4790209a5"
      },
      "source": [
        "# Load pre-trained weights-\n",
        "# trained_model.load_state_dict(torch.load(PATH + 'ResNet18_best_trained_loss.pth', map_location = device))\n",
        "trained_model.load_state_dict(torch.load('ResNet18_best_trained_loss.pth', map_location = device))"
      ],
      "id": "meaning-correlation",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9vq-Z-lnlZg",
        "outputId": "587912ce-50f8-4bd6-8eeb-c55b5808708d"
      },
      "source": [
        "device"
      ],
      "id": "V9vq-Z-lnlZg",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "statewide-skirt"
      },
      "source": [
        "# Place model on GPU (if available)-\n",
        "trained_model.to(device)"
      ],
      "id": "statewide-skirt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "expanded-carbon",
        "outputId": "1be2e990-8bee-4223-8d0d-899782683603"
      },
      "source": [
        "print(trained_model)"
      ],
      "id": "expanded-carbon",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "previous-nation"
      },
      "source": [
        ""
      ],
      "id": "previous-nation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "recent-alert"
      },
      "source": [
        ""
      ],
      "id": "recent-alert",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compliant-psychiatry"
      },
      "source": [
        "### Custom absolute values weight pruning:\n",
        "\n",
        "All weights less than a computed _threshold_ should be pruned away (or, set to zero) by subclassing ```prune.BasePruningMethod```. The pruning is: _unstructured, global_ and _iterative_.\n",
        "\n",
        "\n",
        "__Refer__:\n",
        "\n",
        "- [Lei Mao's Pytorch Pruning Tutorial](https://leimao.github.io/blog/PyTorch-Pruning/)\n",
        "\n",
        "- [GitHub repo](https://github.com/leimao/PyTorch-Pruning-Example)"
      ],
      "id": "compliant-psychiatry"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ordinary-valley"
      },
      "source": [
        "class ThresholdPruning(prune.BasePruningMethod):\n",
        "    \"\"\"\n",
        "    Custom class to prune weights less than 'threshold'\n",
        "    \"\"\"\n",
        "    PRUNING_TYPE = \"unstructured\"\n",
        "    '''\n",
        "    'PRUNING_TYPE' can be one of: global, structured or unstructured.\n",
        "    \n",
        "    'global' acts across whole module (e.g. remove 20% of weight with smallest value),\n",
        "    'structured' acts on whole channels/modules.\n",
        "    \n",
        "    We need unstructured as we would like to modify each connection in specific parameter tensor (say 'weight' or 'bias')\n",
        "    '''\n",
        "\n",
        "    def __init__(self, threshold):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def compute_mask(self, tensor, default_mask):\n",
        "        '''\n",
        "        mask to be used to prune specific tensor. In our case all parameters below 'threshold' should be 0. This is done\n",
        "        with absolute value as it makes more sense.\n",
        "        \n",
        "        'default_mask' is not needed here, but is left as named parameter as that's what API requires atm.\n",
        "        '''\n",
        "        return torch.abs(tensor) > self.threshold\n",
        "\n"
      ],
      "id": "ordinary-valley",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boxed-jacket"
      },
      "source": [
        ""
      ],
      "id": "boxed-jacket",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "warming-valuable"
      },
      "source": [
        "def compute_global_threshold(model, p = 15):\n",
        "    '''\n",
        "    Function to compute global threshold for smallest magnitude pth percentile of weights\n",
        "    '''\n",
        "    # Python3 dict to hold weights of trained model-\n",
        "    model_wts = {}\n",
        "    \n",
        "    for layer, params in model.named_parameters():\n",
        "        model_wts[layer] = params.detach().cpu().numpy()\n",
        "    \n",
        "    # Python3 list to hold absolute magnitude flattened weights-\n",
        "    flattened_wts = []\n",
        "    \n",
        "    '''\n",
        "    for layer, params in model.named_parameters():\n",
        "        # print(f\"layer: {layer} has {params.shape}\")\n",
        "        flattened_wts.append(np.abs(params.detach().numpy().flatten()))\n",
        "    '''\n",
        "    \n",
        "    for layer in model_wts.keys():\n",
        "        flattened_wts.append(np.abs(model_wts[layer].flatten()))\n",
        "    \n",
        "    # Compute threshold using all weights from model-\n",
        "    threshold = np.percentile(np.concatenate(flattened_wts), p)\n",
        "    # print(f\"threshold for {p}th percentile = {threshold:.4f}\")\n",
        "    \n",
        "    return threshold\n"
      ],
      "id": "warming-valuable",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "higher-mediterranean"
      },
      "source": [
        ""
      ],
      "id": "higher-mediterranean",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "another-ancient",
        "outputId": "a3de7b68-6b79-4525-a10c-4698d706545d"
      },
      "source": [
        "threshold = compute_global_threshold(trained_model, p = 20)\n",
        "print(f\"threshold for 20th percentile = {threshold}\")"
      ],
      "id": "another-ancient",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "threshold for 20th percentile = 2.931090818947269e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF4pkFBwntyf",
        "outputId": "2875f4da-57de-442c-c692-24a00954856f"
      },
      "source": [
        "threshold = compute_global_threshold(trained_model, p = 90)\n",
        "print(f\"threshold for 20th percentile = {threshold}\")"
      ],
      "id": "CF4pkFBwntyf",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "threshold for 20th percentile = 1.5760478891024846e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "proof-sauce"
      },
      "source": [
        ""
      ],
      "id": "proof-sauce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "involved-gates"
      },
      "source": [
        "def evaluate_model(model, test_loader, device, criterion = None):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "    return eval_loss, eval_accuracy\n"
      ],
      "id": "involved-gates",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "special-mother"
      },
      "source": [
        ""
      ],
      "id": "special-mother",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "important-typing"
      },
      "source": [
        "def create_classification_report(model, device, test_loader):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            y_true += data[1].numpy().tolist()\n",
        "            images, _ = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            y_pred += predicted.cpu().numpy().tolist()\n",
        "\n",
        "    classification_report = sklearn.metrics.classification_report(\n",
        "        y_true = y_true, y_pred = y_pred)\n",
        "\n",
        "    return classification_report\n"
      ],
      "id": "important-typing",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coordinate-fitness"
      },
      "source": [
        ""
      ],
      "id": "coordinate-fitness",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "directed-murder"
      },
      "source": [
        "def remove_parameters(model):\n",
        "\n",
        "    for module_name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            try:\n",
        "                prune.remove(module, \"weight\")\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                prune.remove(module, \"bias\")\n",
        "            except:\n",
        "                pass\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            try:\n",
        "                prune.remove(module, \"weight\")\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                prune.remove(module, \"bias\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    return model\n"
      ],
      "id": "directed-murder",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "passing-hawaii"
      },
      "source": [
        ""
      ],
      "id": "passing-hawaii",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arranged-occupation"
      },
      "source": [
        "def compute_final_pruning_rate(pruning_rate, num_iterations):\n",
        "    '''\n",
        "    A function to compute the final pruning rate for iterative pruning.\n",
        "        Note that this cannot be applied for global pruning rate if the pruning rate is heterogeneous among different layers.\n",
        "    Args:\n",
        "        pruning_rate (float): Pruning rate.\n",
        "        num_iterations (int): Number of iterations.\n",
        "    Returns:\n",
        "        float: Final pruning rate.\n",
        "    '''\n",
        "\n",
        "    final_pruning_rate = 1 - (1 - pruning_rate) ** num_iterations\n",
        "\n",
        "    return final_pruning_rate\n"
      ],
      "id": "arranged-occupation",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "basic-store"
      },
      "source": [
        ""
      ],
      "id": "basic-store",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fundamental-potato"
      },
      "source": [
        "def measure_module_sparsity(module, weight=True, bias=False, use_mask=False):\n",
        "\n",
        "    num_zeros = 0\n",
        "    num_elements = 0\n",
        "\n",
        "    if use_mask == True:\n",
        "        for buffer_name, buffer in module.named_buffers():\n",
        "            if \"weight_mask\" in buffer_name and weight == True:\n",
        "                num_zeros += torch.sum(buffer == 0).item()\n",
        "                num_elements += buffer.nelement()\n",
        "            if \"bias_mask\" in buffer_name and bias == True:\n",
        "                num_zeros += torch.sum(buffer == 0).item()\n",
        "                num_elements += buffer.nelement()\n",
        "    else:\n",
        "        for param_name, param in module.named_parameters():\n",
        "            if \"weight\" in param_name and weight == True:\n",
        "                num_zeros += torch.sum(param == 0).item()\n",
        "                num_elements += param.nelement()\n",
        "            if \"bias\" in param_name and bias == True:\n",
        "                num_zeros += torch.sum(param == 0).item()\n",
        "                num_elements += param.nelement()\n",
        "\n",
        "    sparsity = num_zeros / num_elements\n",
        "\n",
        "    return num_zeros, num_elements, sparsity\n"
      ],
      "id": "fundamental-potato",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alike-tactics"
      },
      "source": [
        ""
      ],
      "id": "alike-tactics",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aging-testing"
      },
      "source": [
        "def measure_global_sparsity(\n",
        "    model, weight = True,\n",
        "    bias = False, conv2d_use_mask = False,\n",
        "    linear_use_mask = False):\n",
        "\n",
        "    num_zeros = 0\n",
        "    num_elements = 0\n",
        "\n",
        "    for module_name, module in model.named_modules():\n",
        "\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "\n",
        "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
        "                module, weight=weight, bias=bias, use_mask=conv2d_use_mask)\n",
        "            num_zeros += module_num_zeros\n",
        "            num_elements += module_num_elements\n",
        "\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "\n",
        "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
        "                module, weight=weight, bias=bias, use_mask=linear_use_mask)\n",
        "            num_zeros += module_num_zeros\n",
        "            num_elements += module_num_elements\n",
        "\n",
        "    sparsity = num_zeros / num_elements\n",
        "\n",
        "    return num_zeros, num_elements, sparsity\n"
      ],
      "id": "aging-testing",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "satellite-benchmark"
      },
      "source": [
        ""
      ],
      "id": "satellite-benchmark",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "victorian-softball"
      },
      "source": [
        ""
      ],
      "id": "victorian-softball",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attempted-silver"
      },
      "source": [
        "### torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False)\n",
        "[refer](https://pytorch.org/docs/stable/optim.html)\n",
        "\n",
        "Decays the learning rate of each parameter group by 'gamma' once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When ```last_epoch = -1```, sets initial lr as lr.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "- optimizer (Optimizer) â€“ Wrapped optimizer.\n",
        "\n",
        "- milestones (list) â€“ List of epoch indices. Must be increasing.\n",
        "\n",
        "- gamma (float) â€“ Multiplicative factor of learning rate decay. Default: 0.1.\n",
        "\n",
        "- last_epoch (int) â€“ The index of last epoch. Default: -1.\n",
        "\n",
        "- verbose (bool) â€“ If True, prints a message to stdout for each update. Default: False.\n",
        "\n",
        "Example:\n",
        "```\n",
        "# Assuming optimizer uses lr = 0.05 for all groups\n",
        ">>> # lr = 0.05     if epoch < 30\n",
        ">>> # lr = 0.005    if 30 <= epoch < 80\n",
        ">>> # lr = 0.0005   if epoch >= 80\n",
        ">>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
        ">>> for epoch in range(100):\n",
        ">>>     train(...)\n",
        ">>>     validate(...)\n",
        ">>>     scheduler.step()\n",
        "```"
      ],
      "id": "attempted-silver"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "romantic-chinese"
      },
      "source": [
        "def fine_tune_train_model(model, train_loader, test_loader, device, l1_regularization_strength = 0,\n",
        "                l2_regularization_strength = 1e-4, learning_rate = 1e-1, num_epochs = 20):\n",
        "\n",
        "    # The training configurations were not carefully selected.\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10-\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(), lr = learning_rate,\n",
        "        momentum = 0.9, weight_decay = l2_regularization_strength\n",
        "    )\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "    \n",
        "    # Define learning rate scheduler-\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "        # optimizer, milestones = [100, 150],\n",
        "        optimizer, milestones = [5, 10],\n",
        "        gamma = 0.1, last_epoch = -1)\n",
        "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
        "    \n",
        "\n",
        "    # Evaluation-\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = evaluate_model(\n",
        "        model = model, test_loader = test_loader,\n",
        "        device = device, criterion = criterion)\n",
        "    \n",
        "    print(f\"Pre fine-tuning: val_loss = {eval_loss:.3f} & val_accuracy = {eval_accuracy * 100:.3f}%\")\n",
        "    # print(\"Epoch: {:03d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(0, eval_loss, eval_accuracy))\n",
        "\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            l1_reg = torch.tensor(0.).to(device)\n",
        "            for module in model.modules():\n",
        "                mask = None\n",
        "                weight = None\n",
        "                for name, buffer in module.named_buffers():\n",
        "                    if name == \"weight_mask\":\n",
        "                        mask = buffer\n",
        "                for name, param in module.named_parameters():\n",
        "                    if name == \"weight_orig\":\n",
        "                        weight = param\n",
        "                # We usually only want to introduce sparsity to weights and prune weights.\n",
        "                # Do the same for bias if necessary.\n",
        "                if mask is not None and weight is not None:\n",
        "                    l1_reg += torch.norm(mask * weight, 1)\n",
        "\n",
        "            loss += l1_regularization_strength * l1_reg\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(\n",
        "            model = model, test_loader = test_loader,\n",
        "            device = device, criterion = criterion)\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        '''\n",
        "        print(\n",
        "            \"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\"\n",
        "            .format(epoch + 1, train_loss, train_accuracy, eval_loss,\n",
        "                    eval_accuracy))\n",
        "        '''\n",
        "        print(f\"epoch = {epoch + 1} loss = {train_loss:.3f}, accuracy = {train_accuracy * 100:.3f}%, val_loss = {eval_loss:.3f}, val_accuracy = {eval_accuracy * 100:.3f}% & LR: {optimizer.param_groups[0]['lr']:.4f}\")\n",
        "\n",
        "    return model\n"
      ],
      "id": "romantic-chinese",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phantom-isaac"
      },
      "source": [
        "# Sanity check-\n",
        "# fine_tuned_model = fine_tune_train_model(trained_model, train_loader, test_loader, device)"
      ],
      "id": "phantom-isaac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "judicial-rendering"
      },
      "source": [
        ""
      ],
      "id": "judicial-rendering",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "romance-pension"
      },
      "source": [
        ""
      ],
      "id": "romance-pension",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mighty-istanbul"
      },
      "source": [
        "### Available Pruning Methods:\n",
        "\n",
        "The following child classes inherit from the ```BasePruningMethod```:\n",
        "\n",
        "- ```torch.nn.utils.prune.Identity```: utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones;\n",
        "\n",
        "- ```torch.nn.utils.prune.RandomUnstructured```: prune (currently unpruned) entries in a tensor at random;\n",
        "\n",
        "- ```torch.nn.utils.prune.L1Unstructured```: prune (currently unpruned) entries in a tensor __by zeroing out the ones with the lowest absolute magnitude__;\n",
        "\n",
        "- ```torch.nn.utils.prune.RandomStructured```: prune entire (currently unpruned)rows or columns in a tensor random;\n",
        "\n",
        "- ```torch.nn.utils.prune.LnStructured```:  prune entire (currently unpruned) rows or columns in a tensor based on their $L_n$-norm (supported values of _n_ correspond to sup-ported values for argument _p_ in ```torch.norm()```);\n",
        "\n",
        "- ```torch.nn.utils.prune.CustomFromMask```: prune a tensor using a user-provided mask.\n",
        "\n",
        "\n",
        "Refer to \"STREAMLINING TENSOR AND NETWORK PRUNING IN PYTORCH\" by Michela Paganini et al. research paper for more details."
      ],
      "id": "mighty-istanbul"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spectacular-denmark"
      },
      "source": [
        ""
      ],
      "id": "spectacular-denmark",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "global-atlanta"
      },
      "source": [
        "def save_model(model, model_dir, model_filename):\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "        \n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.save(model.state_dict(), model_filepath)\n",
        "    \n",
        "    return None\n"
      ],
      "id": "global-atlanta",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yellow-monte"
      },
      "source": [
        "def load_model(model, model_filepath, device):\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "\n",
        "    return model"
      ],
      "id": "yellow-monte",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rational-capture"
      },
      "source": [
        ""
      ],
      "id": "rational-capture",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gentle-seller"
      },
      "source": [
        ""
      ],
      "id": "gentle-seller",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ba7e87-ab81-414b-b7bf-80d78ea76603"
      },
      "source": [
        "pruning_rates = []"
      ],
      "id": "25ba7e87-ab81-414b-b7bf-80d78ea76603",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac04d9e9-3a7c-4cc2-b914-37b67bd06bdb"
      },
      "source": [
        "# Compute final pruning rate using 20% global unstructured pruning with 21 such pruning rounds-\n",
        "final_pruning_rate = compute_final_pruning_rate(pruning_rate = 0.2, num_iterations = 21)"
      ],
      "id": "ac04d9e9-3a7c-4cc2-b914-37b67bd06bdb",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbe846d0-6f1e-4f40-b23a-2f41fe0ea299"
      },
      "source": [
        "for i in range(1, 22):\n",
        "    final_pruning_rate = compute_final_pruning_rate(pruning_rate = 0.2, num_iterations = i)\n",
        "    pruning_rates.append(final_pruning_rate * 100)\n",
        "    # print(f\"ResNet-18 pruning round = {i} & pruning rate = {final_pruning_rate * 100:.3f}%\")"
      ],
      "id": "bbe846d0-6f1e-4f40-b23a-2f41fe0ea299",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98058de2-150b-4756-aadc-a564435e4b97",
        "outputId": "9a306930-cd11-4e1a-84f1-a3678401b599"
      },
      "source": [
        "pruning_rates"
      ],
      "id": "98058de2-150b-4756-aadc-a564435e4b97",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[19.999999999999996,\n",
              " 35.999999999999986,\n",
              " 48.79999999999999,\n",
              " 59.03999999999999,\n",
              " 67.23199999999999,\n",
              " 73.78559999999999,\n",
              " 79.02847999999999,\n",
              " 83.22278399999999,\n",
              " 86.57822719999999,\n",
              " 89.26258175999999,\n",
              " 91.410065408,\n",
              " 93.1280523264,\n",
              " 94.50244186112,\n",
              " 95.601953488896,\n",
              " 96.48156279111679,\n",
              " 97.18525023289344,\n",
              " 97.74820018631475,\n",
              " 98.1985601490518,\n",
              " 98.55884811924143,\n",
              " 98.84707849539315,\n",
              " 99.07766279631453]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "415ebc55-4248-40ad-b87e-4e030afc3084"
      },
      "source": [
        ""
      ],
      "id": "415ebc55-4248-40ad-b87e-4e030afc3084",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "horizontal-trial"
      },
      "source": [
        "def iterative_pruning_finetuning(\n",
        "    model, train_loader, test_loader, device,\n",
        "    learning_rate, l1_regularization_strength,\n",
        "    l2_regularization_strength, learning_rate_decay = 0.1,\n",
        "    conv2d_prune_amount = 0.2, linear_prune_amount = 0.1,\n",
        "    num_iterations = 10, num_epochs_per_iteration = 10,\n",
        "    model_filename_prefix = \"pruned_model\", model_dir = \"saved_models\",\n",
        "    grouped_pruning = False):\n",
        "    \n",
        "    '''\n",
        "    num_iterations - number of pruning iterations/rounds\n",
        "    num_epochs_per_iteration - number of fine-tuning rounds\n",
        "    '''\n",
        "    \n",
        "    best_val_acc = 0\n",
        "\n",
        "    for i in range(1, num_iterations + 1):\n",
        "\n",
        "        print(\"\\nPruning and Finetuning {}/{}\".format(i, num_iterations))\n",
        "\n",
        "        print(\"Pruning...\")\n",
        "\n",
        "\n",
        "        # NOTE: For global pruning, linear/dense layer can also be pruned!\n",
        "        if grouped_pruning == True:\n",
        "            # grouped_pruning -> Global pruning\n",
        "            parameters_to_prune = []\n",
        "            for module_name, module in model.named_modules():\n",
        "                if isinstance(module, torch.nn.Conv2d):\n",
        "                    parameters_to_prune.append((module, \"weight\"))\n",
        "                elif isinstance(module, torch.nn.Linear):\n",
        "                    parameters_to_prune.append((module, \"weight\"))\n",
        "            \n",
        "            '''\n",
        "            # L1Unstructured - prune (currently unpruned) entries in a tensor by zeroing\n",
        "            # out the ones with the lowest absolute magnitude-\n",
        "            prune.global_unstructured(\n",
        "                parameters_to_prune,\n",
        "                pruning_method = prune.L1Unstructured,\n",
        "                amount = conv2d_prune_amount,\n",
        "            )\n",
        "            '''\n",
        "            \n",
        "            # Use custom absolute magnitude weight based 'global', 'unstructured' and 'iterative' pruning:\n",
        "            \n",
        "            # Compute threshold-\n",
        "            computed_threshold = compute_global_threshold(model, p = pruning_rates[i - 1])\n",
        "            # print(f\"threshold for 20th percentile = {threshold}\")\n",
        "\n",
        "            prune.global_unstructured(\n",
        "                parameters_to_prune,\n",
        "                pruning_method = ThresholdPruning,\n",
        "                threshold = computed_threshold\n",
        "            )\n",
        "        \n",
        "        # layer-wise pruning-\n",
        "        else:\n",
        "            for module_name, module in model.named_modules():\n",
        "                if isinstance(module, torch.nn.Conv2d):\n",
        "                    prune.l1_unstructured(\n",
        "                        module, name = \"weight\",\n",
        "                        amount = conv2d_prune_amount)\n",
        "                elif isinstance(module, torch.nn.Linear):\n",
        "                    prune.l1_unstructured(\n",
        "                        module, name = \"weight\",\n",
        "                        amount = linear_prune_amount)\n",
        "\n",
        "        # Compute validation accuracy just after pruning-\n",
        "        _, eval_accuracy = evaluate_model(\n",
        "            model = model, test_loader = test_loader,\n",
        "            device = device, criterion = None)\n",
        "\n",
        "        '''\n",
        "        classification_report = create_classification_report(\n",
        "            model=model, test_loader=test_loader, device=device)\n",
        "        '''\n",
        "\n",
        "        # Compute global sparsity-\n",
        "        num_zeros, num_elements, sparsity = measure_global_sparsity(\n",
        "            model, weight = True,\n",
        "            bias = False, conv2d_use_mask = True,\n",
        "            linear_use_mask = False)\n",
        "        \n",
        "        print(f\"Global sparsity = {sparsity * 100:.3f}% & val_accuracy = {eval_accuracy * 100:.3f}%\")\n",
        "        # print(model.conv1._forward_pre_hooks)\n",
        "\n",
        "        print(\"\\nFine-tuning...\")\n",
        "\n",
        "        # train_model(\n",
        "        fine_tuned_model = fine_tune_train_model(\n",
        "            model = model, train_loader = train_loader,\n",
        "            test_loader = test_loader, device = device,\n",
        "            l1_regularization_strength = l1_regularization_strength,\n",
        "            l2_regularization_strength = l2_regularization_strength,\n",
        "            # i -> current pruning round-\n",
        "            # learning_rate = learning_rate * (learning_rate_decay ** i),\n",
        "            learning_rate = 1e-1,\n",
        "            num_epochs = num_epochs_per_iteration)\n",
        "\n",
        "        _, eval_accuracy = evaluate_model(\n",
        "            model=model, test_loader = test_loader,\n",
        "            device = device, criterion = None)\n",
        "\n",
        "        '''\n",
        "        classification_report = create_classification_report(\n",
        "            model=model, test_loader=test_loader, device=device)\n",
        "        '''\n",
        "\n",
        "        num_zeros, num_elements, sparsity = measure_global_sparsity(\n",
        "            # model,\n",
        "            model, weight = True,\n",
        "            bias = False, conv2d_use_mask = True,\n",
        "            linear_use_mask = False)\n",
        "\n",
        "        print(f\"Post fine-tuning: Global sparsity = {sparsity * 100:.3f}% & val_accuracy = {eval_accuracy * 100:.3f}%\")\n",
        "\n",
        "        '''\n",
        "        model_filename = \"{}_{}.pt\".format(model_filename_prefix, i + 1)\n",
        "        model_filepath = os.path.join(model_dir, model_filename)\n",
        "        save_model(model=model,\n",
        "                   model_dir=model_dir,\n",
        "                   model_filename=model_filename)\n",
        "        model = load_model(model=model,\n",
        "                           model_filepath=model_filepath,\n",
        "                           device=device)\n",
        "        '''\n",
        "        \n",
        "        '''\n",
        "        # Save best performing model yet-\n",
        "        if eval_accuracy > best_val_acc:\n",
        "            best_val_acc = eval_accuracy\n",
        "            print(f\"\\nSaving ResNet-18 sparsity = {sparsity * 100:.3f}%\")\n",
        "            torch.save(model.state_dict(), f'ResNet18_trained_sparsity-{sparsity * 100:.3f}.pth')\n",
        "        '''\n",
        "        \n",
        "        \n",
        "    return model\n"
      ],
      "id": "horizontal-trial",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "judicial-acquisition"
      },
      "source": [
        ""
      ],
      "id": "judicial-acquisition",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "conditional-league"
      },
      "source": [
        ""
      ],
      "id": "conditional-league",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oriented-gambling"
      },
      "source": [
        "# num_classes = 10\n",
        "# random_seed = 1\n",
        "l1_regularization_strength = 0\n",
        "l2_regularization_strength = 1e-4\n",
        "learning_rate = 1e-3\n",
        "learning_rate_decay = 1"
      ],
      "id": "oriented-gambling",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "freelance-driver"
      },
      "source": [
        ""
      ],
      "id": "freelance-driver",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "equipped-barrel"
      },
      "source": [
        "_, eval_accuracy = evaluate_model(\n",
        "    model = trained_model, test_loader=test_loader,\n",
        "    device = device, criterion = None)"
      ],
      "id": "equipped-barrel",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "danish-perfume"
      },
      "source": [
        "'''\n",
        "classification_report = create_classification_report(\n",
        "    model = trained_model, test_loader = test_loader,\n",
        "    device = device)\n",
        "'''"
      ],
      "id": "danish-perfume",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pretty-anthony",
        "outputId": "65ca19cd-2923-49c3-e14d-17e5114c8b62"
      },
      "source": [
        "num_zeros, num_elements, sparsity = measure_global_sparsity(trained_model)\n",
        "print(f\"Global sparsity = {sparsity:.3f}% & val_accuracy = {eval_accuracy * 100:.3f}%\")\n",
        "\n",
        "# print(\"Test Accuracy: {:.3f}\".format(eval_accuracy))\n",
        "# print(\"Classification Report:\")\n",
        "# print(classification_report)\n",
        "# print(\"Global Sparsity:\")\n",
        "# print(\"{:.2f}\".format(sparsity))"
      ],
      "id": "pretty-anthony",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global sparsity = 0.000% & val_accuracy = 88.990%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cross-immunology",
        "outputId": "69d27975-ddae-4798-ddfe-a66e6a2a5d55"
      },
      "source": [
        "print(f\"ResNet-18 has {num_elements} trainable parameters in it\")"
      ],
      "id": "cross-immunology",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet-18 has 11164352 trainable parameters in it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "decreased-exhibition"
      },
      "source": [
        ""
      ],
      "id": "decreased-exhibition",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unnecessary-california"
      },
      "source": [
        ""
      ],
      "id": "unnecessary-california",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joint-empty"
      },
      "source": [
        "model_dir = \"saved_models\"\n",
        "model_filename = \"resnet18_cifar10.pth\"\n",
        "model_filename_prefix = \"pruned_model\"\n",
        "pruned_model_filename = \"resnet18_pruned_cifar10.pth\"\n",
        "model_filepath = os.path.join(model_dir, model_filename)\n",
        "pruned_model_filepath = os.path.join(model_dir, pruned_model_filename)\n"
      ],
      "id": "joint-empty",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quarterly-knowing"
      },
      "source": [
        ""
      ],
      "id": "quarterly-knowing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cathedral-butter",
        "outputId": "40cea68e-ecbf-4f70-87d0-b51c298d28a0"
      },
      "source": [
        "print(f\"ResNet-18 final pruning rate = {final_pruning_rate * 100:.3f}%\")"
      ],
      "id": "cathedral-butter",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet-18 final pruning rate = 99.078%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geographic-queen"
      },
      "source": [
        ""
      ],
      "id": "geographic-queen",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "intense-connection"
      },
      "source": [
        ""
      ],
      "id": "intense-connection",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "formal-master"
      },
      "source": [
        "import copy"
      ],
      "id": "formal-master",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "natural-industry",
        "outputId": "35417c52-f1ff-4cb2-d88d-f9f26f2f354c"
      },
      "source": [
        "print(\"Iterative Pruning + Fine-Tuning...\")"
      ],
      "id": "natural-industry",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterative Pruning + Fine-Tuning...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usual-washington",
        "outputId": "14c85de4-eb4c-43b3-b033-d7b0ccff0068"
      },
      "source": [
        "# Create a deep copy of the pre-trained model-\n",
        "pruned_model = copy.deepcopy(trained_model)\n",
        "\n",
        "\n",
        "# Prune and fine-tune trained model-\n",
        "'''\n",
        "num_iterations - number of pruning iterations/rounds\n",
        "num_epochs_per_iteration - number of fine-tuning rounds\n",
        "\n",
        "NOTE:\n",
        "ResNt-18 when pruned @ 20% per global pruning round for 21 such rounds leads to 99.078% global sparsity.\n",
        "'''\n",
        "pruned_model = iterative_pruning_finetuning(\n",
        "        model = pruned_model, train_loader = train_loader,\n",
        "        test_loader = test_loader, device = device,\n",
        "        learning_rate = learning_rate, learning_rate_decay = learning_rate_decay,\n",
        "        l1_regularization_strength = l1_regularization_strength, l2_regularization_strength = l2_regularization_strength,\n",
        "        conv2d_prune_amount = 0.2, linear_prune_amount = 0.1,\n",
        "        num_iterations = 21, num_epochs_per_iteration = 12,\n",
        "        model_filename_prefix = model_filename_prefix, model_dir = model_dir,\n",
        "        grouped_pruning = True)\n",
        "\n",
        "\n",
        "# Apply pruned mask to the parameters/weights and remove the masks-\n",
        "remove_parameters(model = pruned_model)\n",
        "\n",
        "_, eval_accuracy = evaluate_model(\n",
        "    model = pruned_model, test_loader = test_loader,\n",
        "    device = device, criterion = None\n",
        ")\n",
        "\n",
        "'''\n",
        "classification_report = create_classification_report(\n",
        "    model = pruned_model, test_loader = test_loader,\n",
        "    device = device)\n",
        "'''\n",
        "\n",
        "num_zeros, num_elements, sparsity = measure_global_sparsity(pruned_model)\n",
        "\n",
        "\n",
        "print(f\"Global sparsity = {sparsity:.3f} & val_accuracy = {eval_accuracy:.3f}\")\n",
        "# print(\"Classification Report:\")\n",
        "# print(classification_report)\n",
        "# NOTE: classification report is avoided as it's too verbose!\n"
      ],
      "id": "usual-washington",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Pruning and Finetuning 2/21\n",
            "Pruning...\n",
            "Global sparsity = 20.016% & val_accuracy = 88.990%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.355 & val_accuracy = 88.990%\n",
            "epoch = 1 loss = 0.577, accuracy = 80.102%, val_loss = 0.619, val_accuracy = 80.000% & LR: 0.1000\n",
            "epoch = 2 loss = 0.487, accuracy = 83.072%, val_loss = 0.527, val_accuracy = 82.470% & LR: 0.1000\n",
            "epoch = 3 loss = 0.449, accuracy = 84.490%, val_loss = 0.547, val_accuracy = 82.460% & LR: 0.1000\n",
            "epoch = 4 loss = 0.429, accuracy = 85.132%, val_loss = 0.511, val_accuracy = 82.690% & LR: 0.1000\n",
            "epoch = 5 loss = 0.409, accuracy = 85.854%, val_loss = 0.469, val_accuracy = 84.380% & LR: 0.0100\n",
            "epoch = 6 loss = 0.298, accuracy = 89.846%, val_loss = 0.359, val_accuracy = 88.040% & LR: 0.0100\n",
            "epoch = 7 loss = 0.265, accuracy = 90.762%, val_loss = 0.350, val_accuracy = 88.310% & LR: 0.0100\n",
            "epoch = 8 loss = 0.253, accuracy = 91.182%, val_loss = 0.349, val_accuracy = 88.740% & LR: 0.0100\n",
            "epoch = 9 loss = 0.246, accuracy = 91.512%, val_loss = 0.347, val_accuracy = 88.830% & LR: 0.0100\n",
            "epoch = 10 loss = 0.235, accuracy = 91.844%, val_loss = 0.347, val_accuracy = 88.780% & LR: 0.0010\n",
            "epoch = 11 loss = 0.225, accuracy = 92.144%, val_loss = 0.346, val_accuracy = 88.940% & LR: 0.0010\n",
            "epoch = 12 loss = 0.224, accuracy = 92.192%, val_loss = 0.341, val_accuracy = 88.950% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 20.016% & val_accuracy = 88.950%\n",
            "\n",
            "Pruning and Finetuning 3/21\n",
            "Pruning...\n",
            "Global sparsity = 36.030% & val_accuracy = 88.950%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.341 & val_accuracy = 88.950%\n",
            "epoch = 1 loss = 0.382, accuracy = 86.832%, val_loss = 0.501, val_accuracy = 83.370% & LR: 0.1000\n",
            "epoch = 2 loss = 0.377, accuracy = 86.864%, val_loss = 0.458, val_accuracy = 84.980% & LR: 0.1000\n",
            "epoch = 3 loss = 0.364, accuracy = 87.398%, val_loss = 0.463, val_accuracy = 84.610% & LR: 0.1000\n",
            "epoch = 4 loss = 0.361, accuracy = 87.348%, val_loss = 0.496, val_accuracy = 84.210% & LR: 0.1000\n",
            "epoch = 5 loss = 0.351, accuracy = 87.808%, val_loss = 0.484, val_accuracy = 84.100% & LR: 0.0100\n",
            "epoch = 6 loss = 0.267, accuracy = 90.774%, val_loss = 0.344, val_accuracy = 88.340% & LR: 0.0100\n",
            "epoch = 7 loss = 0.231, accuracy = 91.948%, val_loss = 0.334, val_accuracy = 88.890% & LR: 0.0100\n",
            "epoch = 8 loss = 0.221, accuracy = 92.488%, val_loss = 0.336, val_accuracy = 88.870% & LR: 0.0100\n",
            "epoch = 9 loss = 0.209, accuracy = 92.634%, val_loss = 0.334, val_accuracy = 89.120% & LR: 0.0100\n",
            "epoch = 10 loss = 0.208, accuracy = 92.766%, val_loss = 0.338, val_accuracy = 89.180% & LR: 0.0010\n",
            "epoch = 11 loss = 0.193, accuracy = 93.342%, val_loss = 0.330, val_accuracy = 89.430% & LR: 0.0010\n",
            "epoch = 12 loss = 0.190, accuracy = 93.400%, val_loss = 0.329, val_accuracy = 89.350% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 36.030% & val_accuracy = 89.350%\n",
            "\n",
            "Pruning and Finetuning 4/21\n",
            "Pruning...\n",
            "Global sparsity = 48.840% & val_accuracy = 89.350%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.329 & val_accuracy = 89.350%\n",
            "epoch = 1 loss = 0.347, accuracy = 88.112%, val_loss = 0.549, val_accuracy = 82.950% & LR: 0.1000\n",
            "epoch = 2 loss = 0.342, accuracy = 87.980%, val_loss = 0.453, val_accuracy = 85.460% & LR: 0.1000\n",
            "epoch = 3 loss = 0.336, accuracy = 88.258%, val_loss = 0.475, val_accuracy = 84.810% & LR: 0.1000\n",
            "epoch = 4 loss = 0.328, accuracy = 88.676%, val_loss = 0.440, val_accuracy = 85.410% & LR: 0.1000\n",
            "epoch = 5 loss = 0.323, accuracy = 88.734%, val_loss = 0.468, val_accuracy = 84.770% & LR: 0.0100\n",
            "epoch = 6 loss = 0.238, accuracy = 91.768%, val_loss = 0.333, val_accuracy = 88.730% & LR: 0.0100\n",
            "epoch = 7 loss = 0.210, accuracy = 92.758%, val_loss = 0.326, val_accuracy = 89.340% & LR: 0.0100\n",
            "epoch = 8 loss = 0.197, accuracy = 93.092%, val_loss = 0.327, val_accuracy = 89.380% & LR: 0.0100\n",
            "epoch = 9 loss = 0.187, accuracy = 93.562%, val_loss = 0.326, val_accuracy = 89.680% & LR: 0.0100\n",
            "epoch = 10 loss = 0.184, accuracy = 93.592%, val_loss = 0.325, val_accuracy = 89.590% & LR: 0.0010\n",
            "epoch = 11 loss = 0.171, accuracy = 94.042%, val_loss = 0.321, val_accuracy = 89.670% & LR: 0.0010\n",
            "epoch = 12 loss = 0.169, accuracy = 94.238%, val_loss = 0.319, val_accuracy = 89.820% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 48.840% & val_accuracy = 89.820%\n",
            "\n",
            "Pruning and Finetuning 5/21\n",
            "Pruning...\n",
            "Global sparsity = 59.088% & val_accuracy = 89.820%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.319 & val_accuracy = 89.820%\n",
            "epoch = 1 loss = 0.311, accuracy = 89.156%, val_loss = 0.437, val_accuracy = 85.660% & LR: 0.1000\n",
            "epoch = 2 loss = 0.312, accuracy = 89.010%, val_loss = 0.512, val_accuracy = 84.040% & LR: 0.1000\n",
            "epoch = 3 loss = 0.312, accuracy = 89.068%, val_loss = 0.408, val_accuracy = 86.510% & LR: 0.1000\n",
            "epoch = 4 loss = 0.310, accuracy = 89.218%, val_loss = 0.476, val_accuracy = 84.780% & LR: 0.1000\n",
            "epoch = 5 loss = 0.300, accuracy = 89.542%, val_loss = 0.459, val_accuracy = 84.710% & LR: 0.0100\n",
            "epoch = 6 loss = 0.220, accuracy = 92.472%, val_loss = 0.314, val_accuracy = 89.600% & LR: 0.0100\n",
            "epoch = 7 loss = 0.189, accuracy = 93.560%, val_loss = 0.315, val_accuracy = 89.700% & LR: 0.0100\n",
            "epoch = 8 loss = 0.178, accuracy = 93.842%, val_loss = 0.312, val_accuracy = 89.940% & LR: 0.0100\n",
            "epoch = 9 loss = 0.170, accuracy = 94.180%, val_loss = 0.315, val_accuracy = 89.910% & LR: 0.0100\n",
            "epoch = 10 loss = 0.161, accuracy = 94.320%, val_loss = 0.320, val_accuracy = 89.920% & LR: 0.0010\n",
            "epoch = 11 loss = 0.155, accuracy = 94.528%, val_loss = 0.316, val_accuracy = 89.950% & LR: 0.0010\n",
            "epoch = 12 loss = 0.149, accuracy = 94.822%, val_loss = 0.313, val_accuracy = 90.030% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 59.088% & val_accuracy = 90.030%\n",
            "\n",
            "Pruning and Finetuning 6/21\n",
            "Pruning...\n",
            "Global sparsity = 67.286% & val_accuracy = 90.030%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.313 & val_accuracy = 90.030%\n",
            "epoch = 1 loss = 0.303, accuracy = 89.288%, val_loss = 0.426, val_accuracy = 86.230% & LR: 0.1000\n",
            "epoch = 2 loss = 0.296, accuracy = 89.660%, val_loss = 0.476, val_accuracy = 84.800% & LR: 0.1000\n",
            "epoch = 3 loss = 0.300, accuracy = 89.546%, val_loss = 0.423, val_accuracy = 86.230% & LR: 0.1000\n",
            "epoch = 4 loss = 0.295, accuracy = 89.532%, val_loss = 0.451, val_accuracy = 85.460% & LR: 0.1000\n",
            "epoch = 5 loss = 0.289, accuracy = 89.850%, val_loss = 0.462, val_accuracy = 85.570% & LR: 0.0100\n",
            "epoch = 6 loss = 0.204, accuracy = 92.932%, val_loss = 0.320, val_accuracy = 89.670% & LR: 0.0100\n",
            "epoch = 7 loss = 0.172, accuracy = 94.050%, val_loss = 0.313, val_accuracy = 90.040% & LR: 0.0100\n",
            "epoch = 8 loss = 0.160, accuracy = 94.576%, val_loss = 0.315, val_accuracy = 90.310% & LR: 0.0100\n",
            "epoch = 9 loss = 0.156, accuracy = 94.576%, val_loss = 0.314, val_accuracy = 90.240% & LR: 0.0100\n",
            "epoch = 10 loss = 0.147, accuracy = 94.856%, val_loss = 0.319, val_accuracy = 90.200% & LR: 0.0010\n",
            "epoch = 11 loss = 0.140, accuracy = 95.156%, val_loss = 0.313, val_accuracy = 90.400% & LR: 0.0010\n",
            "epoch = 12 loss = 0.136, accuracy = 95.182%, val_loss = 0.312, val_accuracy = 90.430% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 67.286% & val_accuracy = 90.430%\n",
            "\n",
            "Pruning and Finetuning 7/21\n",
            "Pruning...\n",
            "Global sparsity = 73.843% & val_accuracy = 90.430%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.312 & val_accuracy = 90.430%\n",
            "epoch = 1 loss = 0.287, accuracy = 89.898%, val_loss = 0.435, val_accuracy = 86.400% & LR: 0.1000\n",
            "epoch = 2 loss = 0.283, accuracy = 90.146%, val_loss = 0.434, val_accuracy = 86.040% & LR: 0.1000\n",
            "epoch = 3 loss = 0.280, accuracy = 90.226%, val_loss = 0.439, val_accuracy = 85.810% & LR: 0.1000\n",
            "epoch = 4 loss = 0.279, accuracy = 90.160%, val_loss = 0.453, val_accuracy = 85.650% & LR: 0.1000\n",
            "epoch = 5 loss = 0.279, accuracy = 90.202%, val_loss = 0.399, val_accuracy = 86.530% & LR: 0.0100\n",
            "epoch = 6 loss = 0.197, accuracy = 93.252%, val_loss = 0.306, val_accuracy = 89.910% & LR: 0.0100\n",
            "epoch = 7 loss = 0.164, accuracy = 94.432%, val_loss = 0.310, val_accuracy = 90.110% & LR: 0.0100\n",
            "epoch = 8 loss = 0.152, accuracy = 94.696%, val_loss = 0.308, val_accuracy = 90.250% & LR: 0.0100\n",
            "epoch = 9 loss = 0.146, accuracy = 94.848%, val_loss = 0.310, val_accuracy = 90.110% & LR: 0.0100\n",
            "epoch = 10 loss = 0.138, accuracy = 95.094%, val_loss = 0.310, val_accuracy = 90.330% & LR: 0.0010\n",
            "epoch = 11 loss = 0.129, accuracy = 95.550%, val_loss = 0.310, val_accuracy = 90.320% & LR: 0.0010\n",
            "epoch = 12 loss = 0.125, accuracy = 95.666%, val_loss = 0.305, val_accuracy = 90.470% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 73.843% & val_accuracy = 90.470%\n",
            "\n",
            "Pruning and Finetuning 8/21\n",
            "Pruning...\n",
            "Global sparsity = 79.087% & val_accuracy = 90.470%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.305 & val_accuracy = 90.470%\n",
            "epoch = 1 loss = 0.274, accuracy = 90.354%, val_loss = 0.483, val_accuracy = 85.090% & LR: 0.1000\n",
            "epoch = 2 loss = 0.280, accuracy = 90.180%, val_loss = 0.460, val_accuracy = 85.860% & LR: 0.1000\n",
            "epoch = 3 loss = 0.273, accuracy = 90.442%, val_loss = 0.433, val_accuracy = 86.280% & LR: 0.1000\n",
            "epoch = 4 loss = 0.265, accuracy = 90.738%, val_loss = 0.482, val_accuracy = 85.070% & LR: 0.1000\n",
            "epoch = 5 loss = 0.265, accuracy = 90.700%, val_loss = 0.415, val_accuracy = 86.600% & LR: 0.0100\n",
            "epoch = 6 loss = 0.183, accuracy = 93.752%, val_loss = 0.309, val_accuracy = 89.910% & LR: 0.0100\n",
            "epoch = 7 loss = 0.154, accuracy = 94.670%, val_loss = 0.306, val_accuracy = 90.390% & LR: 0.0100\n",
            "epoch = 8 loss = 0.140, accuracy = 95.240%, val_loss = 0.308, val_accuracy = 90.300% & LR: 0.0100\n",
            "epoch = 9 loss = 0.133, accuracy = 95.350%, val_loss = 0.311, val_accuracy = 90.460% & LR: 0.0100\n",
            "epoch = 10 loss = 0.127, accuracy = 95.570%, val_loss = 0.315, val_accuracy = 90.450% & LR: 0.0010\n",
            "epoch = 11 loss = 0.120, accuracy = 95.868%, val_loss = 0.307, val_accuracy = 90.710% & LR: 0.0010\n",
            "epoch = 12 loss = 0.118, accuracy = 95.934%, val_loss = 0.311, val_accuracy = 90.650% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 79.087% & val_accuracy = 90.650%\n",
            "\n",
            "Pruning and Finetuning 9/21\n",
            "Pruning...\n",
            "Global sparsity = 83.278% & val_accuracy = 90.650%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.311 & val_accuracy = 90.650%\n",
            "epoch = 1 loss = 0.263, accuracy = 90.622%, val_loss = 0.406, val_accuracy = 87.120% & LR: 0.1000\n",
            "epoch = 2 loss = 0.264, accuracy = 90.722%, val_loss = 0.435, val_accuracy = 86.400% & LR: 0.1000\n",
            "epoch = 3 loss = 0.258, accuracy = 90.944%, val_loss = 0.492, val_accuracy = 84.720% & LR: 0.1000\n",
            "epoch = 4 loss = 0.263, accuracy = 90.666%, val_loss = 0.555, val_accuracy = 83.200% & LR: 0.1000\n",
            "epoch = 5 loss = 0.261, accuracy = 90.724%, val_loss = 0.420, val_accuracy = 87.050% & LR: 0.0100\n",
            "epoch = 6 loss = 0.174, accuracy = 93.878%, val_loss = 0.304, val_accuracy = 90.130% & LR: 0.0100\n",
            "epoch = 7 loss = 0.143, accuracy = 95.170%, val_loss = 0.302, val_accuracy = 90.430% & LR: 0.0100\n",
            "epoch = 8 loss = 0.131, accuracy = 95.476%, val_loss = 0.305, val_accuracy = 90.370% & LR: 0.0100\n",
            "epoch = 9 loss = 0.123, accuracy = 95.748%, val_loss = 0.309, val_accuracy = 90.500% & LR: 0.0100\n",
            "epoch = 10 loss = 0.119, accuracy = 95.854%, val_loss = 0.309, val_accuracy = 90.640% & LR: 0.0010\n",
            "epoch = 11 loss = 0.110, accuracy = 96.180%, val_loss = 0.308, val_accuracy = 90.570% & LR: 0.0010\n",
            "epoch = 12 loss = 0.109, accuracy = 96.252%, val_loss = 0.307, val_accuracy = 90.640% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 83.278% & val_accuracy = 90.640%\n",
            "\n",
            "Pruning and Finetuning 10/21\n",
            "Pruning...\n",
            "Global sparsity = 86.623% & val_accuracy = 90.640%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.307 & val_accuracy = 90.640%\n",
            "epoch = 1 loss = 0.259, accuracy = 90.852%, val_loss = 0.519, val_accuracy = 84.560% & LR: 0.1000\n",
            "epoch = 2 loss = 0.254, accuracy = 91.118%, val_loss = 0.412, val_accuracy = 86.650% & LR: 0.1000\n",
            "epoch = 3 loss = 0.256, accuracy = 90.930%, val_loss = 0.411, val_accuracy = 87.200% & LR: 0.1000\n",
            "epoch = 4 loss = 0.255, accuracy = 91.048%, val_loss = 0.422, val_accuracy = 86.360% & LR: 0.1000\n",
            "epoch = 5 loss = 0.257, accuracy = 90.958%, val_loss = 0.424, val_accuracy = 86.820% & LR: 0.0100\n",
            "epoch = 6 loss = 0.170, accuracy = 94.166%, val_loss = 0.301, val_accuracy = 90.560% & LR: 0.0100\n",
            "epoch = 7 loss = 0.139, accuracy = 95.272%, val_loss = 0.299, val_accuracy = 90.510% & LR: 0.0100\n",
            "epoch = 8 loss = 0.128, accuracy = 95.614%, val_loss = 0.305, val_accuracy = 90.670% & LR: 0.0100\n",
            "epoch = 9 loss = 0.119, accuracy = 95.878%, val_loss = 0.307, val_accuracy = 90.790% & LR: 0.0100\n",
            "epoch = 10 loss = 0.111, accuracy = 96.254%, val_loss = 0.308, val_accuracy = 91.060% & LR: 0.0010\n",
            "epoch = 11 loss = 0.106, accuracy = 96.342%, val_loss = 0.303, val_accuracy = 91.010% & LR: 0.0010\n",
            "epoch = 12 loss = 0.101, accuracy = 96.590%, val_loss = 0.303, val_accuracy = 91.160% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 86.623% & val_accuracy = 91.160%\n",
            "\n",
            "Pruning and Finetuning 11/21\n",
            "Pruning...\n",
            "Global sparsity = 89.276% & val_accuracy = 91.160%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.303 & val_accuracy = 91.160%\n",
            "epoch = 1 loss = 0.248, accuracy = 91.300%, val_loss = 0.426, val_accuracy = 86.030% & LR: 0.1000\n",
            "epoch = 2 loss = 0.259, accuracy = 90.864%, val_loss = 0.413, val_accuracy = 86.770% & LR: 0.1000\n",
            "epoch = 3 loss = 0.246, accuracy = 91.430%, val_loss = 0.446, val_accuracy = 86.540% & LR: 0.1000\n",
            "epoch = 4 loss = 0.245, accuracy = 91.370%, val_loss = 0.398, val_accuracy = 87.810% & LR: 0.1000\n",
            "epoch = 5 loss = 0.239, accuracy = 91.452%, val_loss = 0.444, val_accuracy = 86.290% & LR: 0.0100\n",
            "epoch = 6 loss = 0.155, accuracy = 94.660%, val_loss = 0.311, val_accuracy = 90.280% & LR: 0.0100\n",
            "epoch = 7 loss = 0.130, accuracy = 95.520%, val_loss = 0.308, val_accuracy = 90.620% & LR: 0.0100\n",
            "epoch = 8 loss = 0.119, accuracy = 95.998%, val_loss = 0.307, val_accuracy = 90.850% & LR: 0.0100\n",
            "epoch = 9 loss = 0.109, accuracy = 96.278%, val_loss = 0.307, val_accuracy = 90.720% & LR: 0.0100\n",
            "epoch = 10 loss = 0.103, accuracy = 96.490%, val_loss = 0.311, val_accuracy = 90.900% & LR: 0.0010\n",
            "epoch = 11 loss = 0.095, accuracy = 96.732%, val_loss = 0.310, val_accuracy = 90.940% & LR: 0.0010\n",
            "epoch = 12 loss = 0.091, accuracy = 96.870%, val_loss = 0.310, val_accuracy = 90.960% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 89.276% & val_accuracy = 90.960%\n",
            "\n",
            "Pruning and Finetuning 12/21\n",
            "Pruning...\n",
            "Global sparsity = 91.414% & val_accuracy = 90.960%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.310 & val_accuracy = 90.960%\n",
            "epoch = 1 loss = 0.243, accuracy = 91.422%, val_loss = 0.418, val_accuracy = 86.190% & LR: 0.1000\n",
            "epoch = 2 loss = 0.240, accuracy = 91.636%, val_loss = 0.418, val_accuracy = 87.110% & LR: 0.1000\n",
            "epoch = 3 loss = 0.244, accuracy = 91.442%, val_loss = 0.399, val_accuracy = 87.430% & LR: 0.1000\n",
            "epoch = 4 loss = 0.236, accuracy = 91.734%, val_loss = 0.446, val_accuracy = 86.320% & LR: 0.1000\n",
            "epoch = 5 loss = 0.242, accuracy = 91.484%, val_loss = 0.372, val_accuracy = 88.220% & LR: 0.0100\n",
            "epoch = 6 loss = 0.152, accuracy = 94.760%, val_loss = 0.293, val_accuracy = 90.660% & LR: 0.0100\n",
            "epoch = 7 loss = 0.126, accuracy = 95.594%, val_loss = 0.293, val_accuracy = 90.980% & LR: 0.0100\n",
            "epoch = 8 loss = 0.111, accuracy = 96.214%, val_loss = 0.294, val_accuracy = 90.950% & LR: 0.0100\n",
            "epoch = 9 loss = 0.104, accuracy = 96.462%, val_loss = 0.304, val_accuracy = 90.970% & LR: 0.0100\n",
            "epoch = 10 loss = 0.099, accuracy = 96.632%, val_loss = 0.305, val_accuracy = 91.060% & LR: 0.0010\n",
            "epoch = 11 loss = 0.093, accuracy = 96.828%, val_loss = 0.301, val_accuracy = 91.180% & LR: 0.0010\n",
            "epoch = 12 loss = 0.090, accuracy = 96.992%, val_loss = 0.302, val_accuracy = 91.210% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 91.414% & val_accuracy = 91.210%\n",
            "\n",
            "Pruning and Finetuning 13/21\n",
            "Pruning...\n",
            "Global sparsity = 93.128% & val_accuracy = 91.220%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.302 & val_accuracy = 91.220%\n",
            "epoch = 1 loss = 0.235, accuracy = 91.598%, val_loss = 0.409, val_accuracy = 87.100% & LR: 0.1000\n",
            "epoch = 2 loss = 0.233, accuracy = 91.700%, val_loss = 0.380, val_accuracy = 88.320% & LR: 0.1000\n",
            "epoch = 3 loss = 0.229, accuracy = 91.970%, val_loss = 0.485, val_accuracy = 85.770% & LR: 0.1000\n",
            "epoch = 4 loss = 0.233, accuracy = 91.650%, val_loss = 0.452, val_accuracy = 86.880% & LR: 0.1000\n",
            "epoch = 5 loss = 0.235, accuracy = 91.848%, val_loss = 0.412, val_accuracy = 87.140% & LR: 0.0100\n",
            "epoch = 6 loss = 0.148, accuracy = 94.966%, val_loss = 0.304, val_accuracy = 90.420% & LR: 0.0100\n",
            "epoch = 7 loss = 0.117, accuracy = 95.972%, val_loss = 0.294, val_accuracy = 90.900% & LR: 0.0100\n",
            "epoch = 8 loss = 0.106, accuracy = 96.346%, val_loss = 0.297, val_accuracy = 90.950% & LR: 0.0100\n",
            "epoch = 9 loss = 0.097, accuracy = 96.620%, val_loss = 0.302, val_accuracy = 90.970% & LR: 0.0100\n",
            "epoch = 10 loss = 0.092, accuracy = 96.794%, val_loss = 0.306, val_accuracy = 91.070% & LR: 0.0010\n",
            "epoch = 11 loss = 0.085, accuracy = 97.068%, val_loss = 0.304, val_accuracy = 91.260% & LR: 0.0010\n",
            "epoch = 12 loss = 0.083, accuracy = 97.156%, val_loss = 0.300, val_accuracy = 91.500% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 93.128% & val_accuracy = 91.500%\n",
            "\n",
            "Pruning and Finetuning 14/21\n",
            "Pruning...\n",
            "Global sparsity = 94.499% & val_accuracy = 91.480%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.300 & val_accuracy = 91.480%\n",
            "epoch = 1 loss = 0.224, accuracy = 92.236%, val_loss = 0.402, val_accuracy = 87.920% & LR: 0.1000\n",
            "epoch = 2 loss = 0.222, accuracy = 92.178%, val_loss = 0.451, val_accuracy = 86.460% & LR: 0.1000\n",
            "epoch = 3 loss = 0.224, accuracy = 92.142%, val_loss = 0.505, val_accuracy = 85.170% & LR: 0.1000\n",
            "epoch = 4 loss = 0.220, accuracy = 92.346%, val_loss = 0.389, val_accuracy = 87.790% & LR: 0.1000\n",
            "epoch = 5 loss = 0.223, accuracy = 92.064%, val_loss = 0.449, val_accuracy = 86.610% & LR: 0.0100\n",
            "epoch = 6 loss = 0.141, accuracy = 95.260%, val_loss = 0.294, val_accuracy = 90.540% & LR: 0.0100\n",
            "epoch = 7 loss = 0.109, accuracy = 96.354%, val_loss = 0.296, val_accuracy = 90.850% & LR: 0.0100\n",
            "epoch = 8 loss = 0.096, accuracy = 96.710%, val_loss = 0.301, val_accuracy = 90.890% & LR: 0.0100\n",
            "epoch = 9 loss = 0.091, accuracy = 96.870%, val_loss = 0.297, val_accuracy = 90.970% & LR: 0.0100\n",
            "epoch = 10 loss = 0.085, accuracy = 97.128%, val_loss = 0.309, val_accuracy = 91.030% & LR: 0.0010\n",
            "epoch = 11 loss = 0.080, accuracy = 97.282%, val_loss = 0.303, val_accuracy = 91.100% & LR: 0.0010\n",
            "epoch = 12 loss = 0.079, accuracy = 97.254%, val_loss = 0.302, val_accuracy = 91.140% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 94.499% & val_accuracy = 91.140%\n",
            "\n",
            "Pruning and Finetuning 15/21\n",
            "Pruning...\n",
            "Global sparsity = 95.596% & val_accuracy = 91.110%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.301 & val_accuracy = 91.110%\n",
            "epoch = 1 loss = 0.207, accuracy = 92.772%, val_loss = 0.470, val_accuracy = 86.650% & LR: 0.1000\n",
            "epoch = 2 loss = 0.211, accuracy = 92.532%, val_loss = 0.440, val_accuracy = 87.720% & LR: 0.1000\n",
            "epoch = 3 loss = 0.215, accuracy = 92.476%, val_loss = 0.406, val_accuracy = 87.710% & LR: 0.1000\n",
            "epoch = 4 loss = 0.207, accuracy = 92.772%, val_loss = 0.439, val_accuracy = 86.840% & LR: 0.1000\n",
            "epoch = 5 loss = 0.214, accuracy = 92.512%, val_loss = 0.386, val_accuracy = 87.630% & LR: 0.0100\n",
            "epoch = 6 loss = 0.129, accuracy = 95.540%, val_loss = 0.290, val_accuracy = 91.020% & LR: 0.0100\n",
            "epoch = 7 loss = 0.099, accuracy = 96.646%, val_loss = 0.287, val_accuracy = 91.040% & LR: 0.0100\n",
            "epoch = 8 loss = 0.090, accuracy = 96.978%, val_loss = 0.292, val_accuracy = 91.340% & LR: 0.0100\n",
            "epoch = 9 loss = 0.083, accuracy = 97.194%, val_loss = 0.288, val_accuracy = 91.390% & LR: 0.0100\n",
            "epoch = 10 loss = 0.079, accuracy = 97.350%, val_loss = 0.293, val_accuracy = 91.530% & LR: 0.0010\n",
            "epoch = 11 loss = 0.072, accuracy = 97.592%, val_loss = 0.294, val_accuracy = 91.460% & LR: 0.0010\n",
            "epoch = 12 loss = 0.069, accuracy = 97.708%, val_loss = 0.290, val_accuracy = 91.550% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 95.596% & val_accuracy = 91.550%\n",
            "\n",
            "Pruning and Finetuning 16/21\n",
            "Pruning...\n",
            "Global sparsity = 96.473% & val_accuracy = 91.470%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.291 & val_accuracy = 91.470%\n",
            "epoch = 1 loss = 0.188, accuracy = 93.272%, val_loss = 0.398, val_accuracy = 87.810% & LR: 0.1000\n",
            "epoch = 2 loss = 0.199, accuracy = 93.034%, val_loss = 0.416, val_accuracy = 87.710% & LR: 0.1000\n",
            "epoch = 3 loss = 0.195, accuracy = 93.120%, val_loss = 0.457, val_accuracy = 86.820% & LR: 0.1000\n",
            "epoch = 4 loss = 0.197, accuracy = 93.030%, val_loss = 0.390, val_accuracy = 88.360% & LR: 0.1000\n",
            "epoch = 5 loss = 0.193, accuracy = 93.242%, val_loss = 0.405, val_accuracy = 87.850% & LR: 0.0100\n",
            "epoch = 6 loss = 0.119, accuracy = 95.890%, val_loss = 0.307, val_accuracy = 90.430% & LR: 0.0100\n",
            "epoch = 7 loss = 0.091, accuracy = 96.928%, val_loss = 0.304, val_accuracy = 90.970% & LR: 0.0100\n",
            "epoch = 8 loss = 0.081, accuracy = 97.268%, val_loss = 0.308, val_accuracy = 90.920% & LR: 0.0100\n",
            "epoch = 9 loss = 0.074, accuracy = 97.520%, val_loss = 0.308, val_accuracy = 91.130% & LR: 0.0100\n",
            "epoch = 10 loss = 0.070, accuracy = 97.638%, val_loss = 0.314, val_accuracy = 91.140% & LR: 0.0010\n",
            "epoch = 11 loss = 0.065, accuracy = 97.806%, val_loss = 0.309, val_accuracy = 91.270% & LR: 0.0010\n",
            "epoch = 12 loss = 0.062, accuracy = 97.944%, val_loss = 0.309, val_accuracy = 91.370% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 96.473% & val_accuracy = 91.370%\n",
            "\n",
            "Pruning and Finetuning 17/21\n",
            "Pruning...\n",
            "Global sparsity = 97.176% & val_accuracy = 91.170%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.312 & val_accuracy = 91.170%\n",
            "epoch = 1 loss = 0.176, accuracy = 93.682%, val_loss = 0.492, val_accuracy = 85.980% & LR: 0.1000\n",
            "epoch = 2 loss = 0.183, accuracy = 93.512%, val_loss = 0.414, val_accuracy = 88.040% & LR: 0.1000\n",
            "epoch = 3 loss = 0.185, accuracy = 93.546%, val_loss = 0.379, val_accuracy = 88.380% & LR: 0.1000\n",
            "epoch = 4 loss = 0.178, accuracy = 93.714%, val_loss = 0.390, val_accuracy = 88.160% & LR: 0.1000\n",
            "epoch = 5 loss = 0.186, accuracy = 93.384%, val_loss = 0.359, val_accuracy = 89.080% & LR: 0.0100\n",
            "epoch = 6 loss = 0.112, accuracy = 96.190%, val_loss = 0.297, val_accuracy = 91.090% & LR: 0.0100\n",
            "epoch = 7 loss = 0.087, accuracy = 97.050%, val_loss = 0.297, val_accuracy = 91.370% & LR: 0.0100\n",
            "epoch = 8 loss = 0.076, accuracy = 97.358%, val_loss = 0.298, val_accuracy = 91.410% & LR: 0.0100\n",
            "epoch = 9 loss = 0.071, accuracy = 97.596%, val_loss = 0.298, val_accuracy = 91.590% & LR: 0.0100\n",
            "epoch = 10 loss = 0.067, accuracy = 97.768%, val_loss = 0.302, val_accuracy = 91.730% & LR: 0.0010\n",
            "epoch = 11 loss = 0.060, accuracy = 97.986%, val_loss = 0.300, val_accuracy = 91.790% & LR: 0.0010\n",
            "epoch = 12 loss = 0.059, accuracy = 98.016%, val_loss = 0.299, val_accuracy = 91.770% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 97.176% & val_accuracy = 91.770%\n",
            "\n",
            "Pruning and Finetuning 18/21\n",
            "Pruning...\n",
            "Global sparsity = 97.738% & val_accuracy = 91.590%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.300 & val_accuracy = 91.590%\n",
            "epoch = 1 loss = 0.170, accuracy = 93.964%, val_loss = 0.462, val_accuracy = 86.960% & LR: 0.1000\n",
            "epoch = 2 loss = 0.174, accuracy = 93.796%, val_loss = 0.469, val_accuracy = 86.800% & LR: 0.1000\n",
            "epoch = 3 loss = 0.169, accuracy = 93.978%, val_loss = 0.409, val_accuracy = 88.090% & LR: 0.1000\n",
            "epoch = 4 loss = 0.171, accuracy = 93.892%, val_loss = 0.390, val_accuracy = 88.410% & LR: 0.1000\n",
            "epoch = 5 loss = 0.171, accuracy = 93.984%, val_loss = 0.429, val_accuracy = 87.570% & LR: 0.0100\n",
            "epoch = 6 loss = 0.107, accuracy = 96.402%, val_loss = 0.292, val_accuracy = 91.190% & LR: 0.0100\n",
            "epoch = 7 loss = 0.082, accuracy = 97.344%, val_loss = 0.291, val_accuracy = 91.180% & LR: 0.0100\n",
            "epoch = 8 loss = 0.071, accuracy = 97.656%, val_loss = 0.297, val_accuracy = 91.290% & LR: 0.0100\n",
            "epoch = 9 loss = 0.064, accuracy = 97.822%, val_loss = 0.306, val_accuracy = 91.420% & LR: 0.0100\n",
            "epoch = 10 loss = 0.063, accuracy = 97.926%, val_loss = 0.306, val_accuracy = 91.380% & LR: 0.0010\n",
            "epoch = 11 loss = 0.057, accuracy = 98.160%, val_loss = 0.307, val_accuracy = 91.410% & LR: 0.0010\n",
            "epoch = 12 loss = 0.057, accuracy = 98.188%, val_loss = 0.306, val_accuracy = 91.480% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 97.738% & val_accuracy = 91.480%\n",
            "\n",
            "Pruning and Finetuning 19/21\n",
            "Pruning...\n",
            "Global sparsity = 98.187% & val_accuracy = 91.150%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.310 & val_accuracy = 91.150%\n",
            "epoch = 1 loss = 0.162, accuracy = 94.184%, val_loss = 0.409, val_accuracy = 88.200% & LR: 0.1000\n",
            "epoch = 2 loss = 0.168, accuracy = 94.010%, val_loss = 0.395, val_accuracy = 88.290% & LR: 0.1000\n",
            "epoch = 3 loss = 0.167, accuracy = 94.084%, val_loss = 0.413, val_accuracy = 88.390% & LR: 0.1000\n",
            "epoch = 4 loss = 0.169, accuracy = 93.972%, val_loss = 0.499, val_accuracy = 86.240% & LR: 0.1000\n",
            "epoch = 5 loss = 0.165, accuracy = 94.152%, val_loss = 0.427, val_accuracy = 87.610% & LR: 0.0100\n",
            "epoch = 6 loss = 0.102, accuracy = 96.530%, val_loss = 0.291, val_accuracy = 91.180% & LR: 0.0100\n",
            "epoch = 7 loss = 0.080, accuracy = 97.254%, val_loss = 0.291, val_accuracy = 91.360% & LR: 0.0100\n",
            "epoch = 8 loss = 0.072, accuracy = 97.570%, val_loss = 0.294, val_accuracy = 91.570% & LR: 0.0100\n",
            "epoch = 9 loss = 0.067, accuracy = 97.804%, val_loss = 0.298, val_accuracy = 91.480% & LR: 0.0100\n",
            "epoch = 10 loss = 0.061, accuracy = 97.924%, val_loss = 0.302, val_accuracy = 91.500% & LR: 0.0010\n",
            "epoch = 11 loss = 0.056, accuracy = 98.104%, val_loss = 0.299, val_accuracy = 91.580% & LR: 0.0010\n",
            "epoch = 12 loss = 0.055, accuracy = 98.176%, val_loss = 0.301, val_accuracy = 91.590% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 98.187% & val_accuracy = 91.590%\n",
            "\n",
            "Pruning and Finetuning 20/21\n",
            "Pruning...\n",
            "Global sparsity = 98.546% & val_accuracy = 90.900%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.324 & val_accuracy = 90.900%\n",
            "epoch = 1 loss = 0.164, accuracy = 94.218%, val_loss = 0.415, val_accuracy = 88.480% & LR: 0.1000\n",
            "epoch = 2 loss = 0.170, accuracy = 93.900%, val_loss = 0.391, val_accuracy = 88.340% & LR: 0.1000\n",
            "epoch = 3 loss = 0.164, accuracy = 94.120%, val_loss = 0.463, val_accuracy = 86.840% & LR: 0.1000\n",
            "epoch = 4 loss = 0.166, accuracy = 94.156%, val_loss = 0.445, val_accuracy = 87.700% & LR: 0.1000\n",
            "epoch = 5 loss = 0.168, accuracy = 93.946%, val_loss = 0.498, val_accuracy = 86.370% & LR: 0.0100\n",
            "epoch = 6 loss = 0.105, accuracy = 96.506%, val_loss = 0.302, val_accuracy = 91.030% & LR: 0.0100\n",
            "epoch = 7 loss = 0.081, accuracy = 97.186%, val_loss = 0.299, val_accuracy = 91.410% & LR: 0.0100\n",
            "epoch = 8 loss = 0.073, accuracy = 97.508%, val_loss = 0.304, val_accuracy = 91.460% & LR: 0.0100\n",
            "epoch = 9 loss = 0.068, accuracy = 97.718%, val_loss = 0.307, val_accuracy = 91.480% & LR: 0.0100\n",
            "epoch = 10 loss = 0.062, accuracy = 97.908%, val_loss = 0.307, val_accuracy = 91.580% & LR: 0.0010\n",
            "epoch = 11 loss = 0.057, accuracy = 98.126%, val_loss = 0.308, val_accuracy = 91.500% & LR: 0.0010\n",
            "epoch = 12 loss = 0.058, accuracy = 98.040%, val_loss = 0.304, val_accuracy = 91.770% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 98.546% & val_accuracy = 91.770%\n",
            "\n",
            "Pruning and Finetuning 21/21\n",
            "Pruning...\n",
            "Global sparsity = 98.833% & val_accuracy = 90.420%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.328 & val_accuracy = 90.420%\n",
            "epoch = 1 loss = 0.174, accuracy = 93.918%, val_loss = 0.405, val_accuracy = 88.240% & LR: 0.1000\n",
            "epoch = 2 loss = 0.179, accuracy = 93.592%, val_loss = 0.503, val_accuracy = 86.710% & LR: 0.1000\n",
            "epoch = 3 loss = 0.173, accuracy = 93.880%, val_loss = 0.446, val_accuracy = 87.150% & LR: 0.1000\n",
            "epoch = 4 loss = 0.166, accuracy = 94.132%, val_loss = 0.464, val_accuracy = 86.830% & LR: 0.1000\n",
            "epoch = 5 loss = 0.167, accuracy = 94.134%, val_loss = 0.446, val_accuracy = 87.400% & LR: 0.0100\n",
            "epoch = 6 loss = 0.109, accuracy = 96.276%, val_loss = 0.321, val_accuracy = 90.910% & LR: 0.0100\n",
            "epoch = 7 loss = 0.085, accuracy = 97.152%, val_loss = 0.316, val_accuracy = 91.050% & LR: 0.0100\n",
            "epoch = 8 loss = 0.076, accuracy = 97.462%, val_loss = 0.320, val_accuracy = 91.270% & LR: 0.0100\n",
            "epoch = 9 loss = 0.074, accuracy = 97.506%, val_loss = 0.314, val_accuracy = 91.370% & LR: 0.0100\n",
            "epoch = 10 loss = 0.067, accuracy = 97.664%, val_loss = 0.318, val_accuracy = 91.440% & LR: 0.0010\n",
            "epoch = 11 loss = 0.063, accuracy = 97.942%, val_loss = 0.318, val_accuracy = 91.410% & LR: 0.0010\n",
            "epoch = 12 loss = 0.061, accuracy = 97.988%, val_loss = 0.317, val_accuracy = 91.510% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 98.833% & val_accuracy = 91.510%\n",
            "\n",
            "Pruning and Finetuning 22/21\n",
            "Pruning...\n",
            "Global sparsity = 99.063% & val_accuracy = 90.350%\n",
            "\n",
            "Fine-tuning...\n",
            "Pre fine-tuning: val_loss = 0.351 & val_accuracy = 90.350%\n",
            "epoch = 1 loss = 0.182, accuracy = 93.470%, val_loss = 0.418, val_accuracy = 88.090% & LR: 0.1000\n",
            "epoch = 2 loss = 0.178, accuracy = 93.546%, val_loss = 0.406, val_accuracy = 87.920% & LR: 0.1000\n",
            "epoch = 3 loss = 0.184, accuracy = 93.650%, val_loss = 0.400, val_accuracy = 88.250% & LR: 0.1000\n",
            "epoch = 4 loss = 0.180, accuracy = 93.646%, val_loss = 0.460, val_accuracy = 86.820% & LR: 0.1000\n",
            "epoch = 5 loss = 0.173, accuracy = 93.812%, val_loss = 0.428, val_accuracy = 87.960% & LR: 0.0100\n",
            "epoch = 6 loss = 0.114, accuracy = 96.084%, val_loss = 0.314, val_accuracy = 90.490% & LR: 0.0100\n",
            "epoch = 7 loss = 0.092, accuracy = 96.952%, val_loss = 0.310, val_accuracy = 91.120% & LR: 0.0100\n",
            "epoch = 8 loss = 0.085, accuracy = 97.114%, val_loss = 0.313, val_accuracy = 91.080% & LR: 0.0100\n",
            "epoch = 9 loss = 0.078, accuracy = 97.436%, val_loss = 0.313, val_accuracy = 91.150% & LR: 0.0100\n",
            "epoch = 10 loss = 0.076, accuracy = 97.428%, val_loss = 0.312, val_accuracy = 91.210% & LR: 0.0010\n",
            "epoch = 11 loss = 0.070, accuracy = 97.632%, val_loss = 0.312, val_accuracy = 91.140% & LR: 0.0010\n",
            "epoch = 12 loss = 0.068, accuracy = 97.700%, val_loss = 0.313, val_accuracy = 91.260% & LR: 0.0010\n",
            "Post fine-tuning: Global sparsity = 99.063% & val_accuracy = 91.260%\n",
            "Global sparsity = 0.991 & val_accuracy = 0.913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amended-phase"
      },
      "source": [
        ""
      ],
      "id": "amended-phase",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "breeding-freeware"
      },
      "source": [
        "# Remove pruning parameters-\n",
        "final_model = remove_parameters(pruned_model)"
      ],
      "id": "breeding-freeware",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "steady-frost",
        "outputId": "cc37cf77-f39f-4f35-a2ef-19ff34ed8534"
      },
      "source": [
        "# Compute final model's val_accuracy and global sparsity-\n",
        "_, eval_accuracy = evaluate_model(\n",
        "    model = final_model, test_loader = test_loader,\n",
        "    device = device, criterion = None)\n",
        "\n",
        "num_zeros, num_elements, sparsity = measure_global_sparsity(pruned_model)\n",
        "print(f\"Global sparsity = {sparsity * 100:.3f}%\"\n",
        "f\" & val_accuracy = {eval_accuracy * 100:.3f}%\")\n"
      ],
      "id": "steady-frost",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global sparsity = 99.099% & val_accuracy = 91.260%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "standing-socket"
      },
      "source": [
        ""
      ],
      "id": "standing-socket",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "completed-color"
      },
      "source": [
        "# Save final trained and pruned model for later use-\n",
        "torch.save(final_model.state_dict(), f\"ResNet18_trained_sparsity-{sparsity * 100:.3f}.pth\")"
      ],
      "id": "completed-color",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arbitrary-zimbabwe"
      },
      "source": [
        ""
      ],
      "id": "arbitrary-zimbabwe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abstract-printing"
      },
      "source": [
        ""
      ],
      "id": "abstract-printing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "starting-ecuador"
      },
      "source": [
        "### Sanity check:"
      ],
      "id": "starting-ecuador"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sharing-employment"
      },
      "source": [
        "# Initialize and load trained and pruned model-\n",
        "trained_pruned_model = ResNet18()\n",
        "trained_pruned_model.load_state_dict(torch.load(\"/content/ResNet18_trained_sparsity-99.099.pth\", map_location = device))\n",
        "# trained_pruned_model.load_state_dict(final_model)\n",
        "\n",
        "# Move model to GPU (if available)-\n",
        "trained_pruned_model.to(device)\n",
        "\n",
        "# Define cost function and optimizer-\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10-\n",
        "optimizer = torch.optim.SGD(\n",
        "        trained_pruned_model.parameters(), lr = learning_rate,\n",
        "        momentum = 0.9, weight_decay = l2_regularization_strength\n",
        ")\n"
      ],
      "id": "sharing-employment",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emotional-franchise",
        "outputId": "40f50f3e-acc7-4955-b3ee-9a09b12ce16c"
      },
      "source": [
        "# Compute final model's val_accuracy and global sparsity-\n",
        "eval_loss, eval_accuracy = evaluate_model(\n",
        "    model = trained_pruned_model, test_loader=test_loader,\n",
        "    device = device, criterion = None)\n",
        "\n",
        "num_zeros, num_elements, sparsity = measure_global_sparsity(trained_pruned_model)\n",
        "print(f\"Global sparsity = {sparsity * 100:.3f}%, val_loss = {eval_loss:.3f}\"\n",
        "f\" & val_accuracy = {eval_accuracy * 100:.3f}%\")\n"
      ],
      "id": "emotional-franchise",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global sparsity = 99.099%, val_loss = 0.000 & val_accuracy = 91.260%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "similar-interference"
      },
      "source": [
        ""
      ],
      "id": "similar-interference",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "union-forge"
      },
      "source": [
        ""
      ],
      "id": "union-forge",
      "execution_count": null,
      "outputs": []
    }
  ]
}