{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e928278f",
   "metadata": {},
   "source": [
    "# Iterative Pruning: Using _torch.nn.utils.prune_\n",
    "\n",
    "Experiment includes 'LeNet-300-100' dense neural network using MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2c31e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab819d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f85553d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7079a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cpu\n"
     ]
    }
   ],
   "source": [
    "# GPU device configuration-\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Available device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508f7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5e37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters-\n",
    "input_size = 784    # 28 x 28, flattened to be 1-D tensor\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.0012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef7d6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset statistics:\n",
    "# mean = tensor([0.1307]) & std dev = tensor([0.3081])\n",
    "mean = np.array([0.1307])\n",
    "std_dev = np.array([0.3081])\n",
    "\n",
    "transforms_apply = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean, std = std_dev)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03dcdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/arjun/Documents/Programs/Python_Codes/PyTorch_Resources/Good_Codes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d7ff871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset): 60000 & len(test_dataset): 10000\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset-\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "        root = './data', train = True,\n",
    "        transform = transforms_apply, download = True\n",
    "        )\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "        root = './data', train = False,\n",
    "        transform = transforms_apply\n",
    "        )\n",
    "\n",
    "\n",
    "print(f\"len(train_dataset): {len(train_dataset)} & len(test_dataset): {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9aac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_loader) = 1875 & len(test_loader) = 313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1875, 313)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataloader-\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset = train_dataset, batch_size = batch_size,\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        dataset = test_dataset, batch_size = batch_size,\n",
    "        shuffle = False\n",
    "        )\n",
    "\n",
    "print(f\"len(train_loader) = {len(train_loader)} & len(test_loader) = {len(test_loader)}\")\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6816015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet300(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet300, self).__init__()\n",
    "        \n",
    "        # Define layers-\n",
    "        self.fc1 = nn.Linear(in_features = input_size, out_features = 300)\n",
    "        self.fc2 = nn.Linear(in_features = 300, out_features = 100)\n",
    "        self.output = nn.Linear(in_features = 100, out_features = 10)\n",
    "        \n",
    "        self.weights_initialization()\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        return self.output(out)\n",
    "    \n",
    "    \n",
    "    def weights_initialization(self):\n",
    "        '''\n",
    "        When we define all the modules such as the layers in '__init__()'\n",
    "        method above, these are all stored in 'self.modules()'.\n",
    "        We go through each module one by one. This is the entire network,\n",
    "        basically.\n",
    "        '''\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4791ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82965eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of LeNet-300-100 dense neural network and load already trained model-\n",
    "best_model = LeNet300()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcdd4ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained weights-\n",
    "best_model.load_state_dict(torch.load(\"/home/arjun/Deep_Learning_Resources/LTH-Resources/LeNet-300-100_Trained.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2340cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba05a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer-\n",
    "loss = nn.CrossEntropyLoss()    # applies softmax for us\n",
    "optimizer = torch.optim.Adam(best_model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3ae0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d01bb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    \n",
    "    tot_params = 0\n",
    "    for layer_name, param in model.named_parameters():\n",
    "        # print(f\"{layer_name}.shape = {param.shape} has {torch.count_nonzero(param.data)} non-zero params\")\n",
    "        tot_params += torch.count_nonzero(param.data)\n",
    "    \n",
    "    return tot_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2938cd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original LeNet-300-100 model has 266610 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "orig_params = count_params(best_model)\n",
    "print(f\"Original LeNet-300-100 model has {orig_params} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a1212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3b2aaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.name: fc1.weight & param.shape = torch.Size([300, 784])\n",
      "layer.name: fc1.bias & param.shape = torch.Size([300])\n",
      "layer.name: fc2.weight & param.shape = torch.Size([100, 300])\n",
      "layer.name: fc2.bias & param.shape = torch.Size([100])\n",
      "layer.name: output.weight & param.shape = torch.Size([10, 100])\n",
      "layer.name: output.bias & param.shape = torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer, param in best_model.named_parameters():\n",
    "    print(f\"layer.name: {layer} & param.shape = {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2472797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'output.weight', 'output.bias'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To access layer names-\n",
    "best_model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b4013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18d26f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_wise_pruning():\n",
    "    '''\n",
    "    Function to compute layer-wise pruning for iterative pruning using LeNet-300-100 model\n",
    "    and MNIST dataset\n",
    "    '''\n",
    "    \n",
    "    # number of fully-connected dense parameters-\n",
    "    dense1 = 235200\n",
    "    dense2 = 30000\n",
    "    op_layer = 100\n",
    "\n",
    "\n",
    "    # total number of parameters-\n",
    "    total_params = dense1 + dense2 + op_layer\n",
    "    # print(f\"Total number of trainable parameters = {total_params}\")\n",
    "    \n",
    "    # maximum pruning performed is till 0.5% of all parameters-\n",
    "    max_pruned_params = 0.005 * total_params\n",
    "        \n",
    "    loc_tot_params = total_params\n",
    "    loc_dense1 = dense1\n",
    "    loc_dense2 = dense2\n",
    "    loc_op_layer = op_layer\n",
    "\n",
    "    # variable to count number of pruning rounds-\n",
    "    n = 0\n",
    "        \n",
    "    # Lists to hold percentage of weights pruned in each round for all layers in NN-\n",
    "    dense1_pruning = []\n",
    "    dense2_pruning = []\n",
    "    op_layer_pruning = []\n",
    "    \n",
    "    \n",
    "    while loc_tot_params >= max_pruned_params:\n",
    "        loc_dense1 *= 0.8   # 20% weights are pruned\n",
    "        loc_dense2 *= 0.8   # 20% weights are pruned\n",
    "        loc_op_layer *= 0.9 # 10% weights are pruned\n",
    "\n",
    "        dense1_pruning.append(((dense1 - loc_dense1) / dense1) * 100)\n",
    "        dense2_pruning.append(((dense2 - loc_dense2) / dense2) * 100)\n",
    "        op_layer_pruning.append(((op_layer - loc_op_layer) / op_layer) * 100)\n",
    "\n",
    "        loc_tot_params = loc_dense1 + loc_dense2 + loc_op_layer\n",
    "\n",
    "        n += 1\n",
    "\n",
    "        '''\n",
    "        print(\"\\nConv1 = {0:.3f}, Conv2 = {1:.3f}, Conv3 = {2:.4f}\".format(loc_conv1, loc_conv2, loc_conv3))\n",
    "        print(\"Conv4 = {0:.3f}, Conv5 = {1:.3f} & Conv6 = {2:.3f}\".format(loc_conv4, loc_conv5, loc_conv6))\n",
    "        print(\"Dense1 = {0:.3f}, Dense2 = {1:.3f} & O/p layer = {2:.3f}\".format(\n",
    "            loc_dense1, loc_dense2, loc_op_layer))\n",
    "        print(\"Total number of parameters = {0:.3f}\\n\".format(loc_tot_params))\n",
    "        '''\n",
    "    \n",
    "    # print(\"\\nnumber of pruning rounds = {0}\\n\\n\".format(n))\n",
    "    \n",
    "    # Convert from list to np.array-\n",
    "    dense1_pruning = np.array(dense1_pruning)\n",
    "    dense2_pruning = np.array(dense2_pruning)\n",
    "    op_layer_pruning = np.array(op_layer_pruning)\n",
    "\n",
    "    # Round off numpy arrays to 3 decimal digits-\n",
    "    dense1_pruning = np.round(dense1_pruning, decimals=3)\n",
    "    dense2_pruning = np.round(dense2_pruning, decimals=3)\n",
    "    op_layer_pruning = np.round(op_layer_pruning, decimals=3)\n",
    "\n",
    "    \n",
    "    # Python 3 dict to hold layer\n",
    "    pruning_layers = {}\n",
    "    \n",
    "    pruning_layers['dense1_pruning'] = dense1_pruning\n",
    "    pruning_layers['dense2_pruning'] = dense2_pruning\n",
    "    pruning_layers['op_layer'] = op_layer_pruning\n",
    "    \n",
    "    return pruning_layers, n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4721814",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_layers, num_pruning_rounds = layer_wise_pruning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d465e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pruning rounds = 24 to achieve 0.5% final sparsity\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of pruning rounds = {num_pruning_rounds} to achieve 0.5% final sparsity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9b80bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.   , 36.   , 48.8  , 59.04 , 67.232, 73.786, 79.028, 83.223,\n",
       "       86.578, 89.263, 91.41 , 93.128, 94.502, 95.602, 96.482, 97.185,\n",
       "       97.748, 98.199, 98.559, 98.847, 99.078, 99.262, 99.41 , 99.528])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check- percentage of weights to prune for first dense layer in each iterative pruning round-\n",
    "pruning_layers['dense1_pruning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af2fe59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.   , 19.   , 27.1  , 34.39 , 40.951, 46.856, 52.17 , 56.953,\n",
       "       61.258, 65.132, 68.619, 71.757, 74.581, 77.123, 79.411, 81.47 ,\n",
       "       83.323, 84.991, 86.491, 87.842, 89.058, 90.152, 91.137, 92.023])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_layers['op_layer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3a112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96421e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e65f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43799f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6595b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader):\n",
    "    '''\n",
    "    Function to perform one epoch of training by using 'train_loader'.\n",
    "    Returns loss and number of correct predictions for this epoch.\n",
    "    '''\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "\n",
    "    for batch, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape image and place it on GPU-\n",
    "        images = images.reshape(-1, input_size).to(device)\n",
    "        labels = labels.to(device) \n",
    "        outputs = model(images)   # forward pass\n",
    "        J = loss(outputs, labels) # compute loss\n",
    "        optimizer.zero_grad()     # empty accumulated gradients\n",
    "        J.backward()              # perform backpropagation\n",
    "        optimizer.step()          # update parameters\n",
    "\n",
    "        # Compute model's performance statistics-\n",
    "        running_loss += J.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(predicted == labels.data)\n",
    "\n",
    "        '''\n",
    "        # Print information every 100 steps-\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f\"epoch {epoch + 1}/{num_epochs}, step {batch + 1}/{num_steps}, loss = {J.item():.4f}\")\n",
    "        '''\n",
    "\n",
    "    return running_loss, running_corrects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df5d1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    running_loss_val = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "\n",
    "            # Place features (images) and targets (labels) to GPU-\n",
    "            images = images.reshape(-1, input_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "            # print(f\"images.shape = {images.shape}, labels.shape = {labels.shape}\")\n",
    "\n",
    "            # Set model to evaluation mode-\n",
    "            model.eval()\n",
    "    \n",
    "            # Make predictions using trained model-\n",
    "            outputs = model(images)\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "\n",
    "            # Compute validation loss-\n",
    "            J_val = loss(outputs, labels)\n",
    "\n",
    "            running_loss_val += J_val.item() * labels.size(0)\n",
    "    \n",
    "            # Total number of labels-\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Total number of correct predictions-\n",
    "            correct += (y_pred == labels).sum()\n",
    "\n",
    "    return (running_loss_val, correct, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf712821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60dd57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "995f4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input parameters for Early Stopping in manual implementation-\n",
    "minimum_delta = 0.001\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fd45c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters for Early Stopping manual implementation-\n",
    "best_val_loss = 100\n",
    "loc_patience = 0\n",
    "\n",
    "# running_loss = 0.0\n",
    "# running_corrects = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8bcc887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 lists to store model training metrics-\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "training_loss = []\n",
    "validation_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d9922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e43a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1 training loss = 0.0428, training accuracy = 98.70%, val_loss = 0.1167 & val_accuracy = 97.01%\n",
      "\n",
      "\n",
      "Saving model with lowest val_loss = 0.1167\n",
      "\n",
      "epoch: 2 training loss = 0.0351, training accuracy = 98.92%, val_loss = 0.1107 & val_accuracy = 97.54%\n",
      "\n",
      "\n",
      "Saving model with lowest val_loss = 0.1107\n",
      "\n",
      "epoch: 3 training loss = 0.0360, training accuracy = 98.92%, val_loss = 0.0979 & val_accuracy = 97.89%\n",
      "\n",
      "\n",
      "Saving model with lowest val_loss = 0.0979\n",
      "\n",
      "epoch: 4 training loss = 0.0303, training accuracy = 99.11%, val_loss = 0.1028 & val_accuracy = 97.83%\n",
      "\n",
      "\n",
      "epoch: 5 training loss = 0.0320, training accuracy = 99.07%, val_loss = 0.1109 & val_accuracy = 97.61%\n",
      "\n",
      "\n",
      "epoch: 6 training loss = 0.0244, training accuracy = 99.31%, val_loss = 0.1262 & val_accuracy = 97.53%\n",
      "\n",
      "\n",
      "epoch: 7 training loss = 0.0270, training accuracy = 99.25%, val_loss = 0.1307 & val_accuracy = 97.47%\n",
      "\n",
      "\n",
      "epoch: 8 training loss = 0.0246, training accuracy = 99.26%, val_loss = 0.1249 & val_accuracy = 97.89%\n",
      "\n",
      "\n",
      "'EarlyStopping' called!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop-\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    \n",
    "    if loc_patience >= patience:\n",
    "        print(\"\\n'EarlyStopping' called!\\n\")\n",
    "        break\n",
    "\n",
    "    running_loss, running_corrects = train_model(best_model, train_loader)\n",
    "  \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "    # epoch_acc = 100 * running_corrects / len(trainset)\n",
    "    # print(f\"\\nepoch: {epoch + 1} training loss = {epoch_loss:.4f}, training accuracy = {epoch_acc * 100:.2f}%\\n\")\n",
    "\n",
    "    running_loss_val, correct, total = test_model(best_model, test_loader)\n",
    "\n",
    "    epoch_val_loss = running_loss_val / len(test_dataset)\n",
    "    val_acc = 100 * (correct / total)\n",
    "    # print(f\"\\nepoch: {epoch + 1} training loss = {epoch_loss:.4f}, training accuracy = {epoch_acc * 100:.2f}%, val_loss = {epoch_val_loss:.4f} & val_accuracy = {val_acc:.2f}%\\n\")\n",
    "\n",
    "    print(f\"\\nepoch: {epoch + 1} training loss = {epoch_loss:.4f}, training accuracy = {epoch_acc * 100:.2f}%, val_loss = {epoch_val_loss:.4f} & val_accuracy = {val_acc:.2f}%\\n\")\n",
    "\n",
    "    # Code for manual Early Stopping:\n",
    "    # if np.abs(epoch_val_loss < best_val_loss) >= minimum_delta:\n",
    "    if (epoch_val_loss < best_val_loss) and np.abs(epoch_val_loss - best_val_loss) >= minimum_delta:\n",
    "        # print(f\"epoch_val_loss = {epoch_val_loss:.4f}, best_val_loss = {best_val_loss:.4f}\")\n",
    "        \n",
    "        # update 'best_val_loss' variable to lowest loss encountered so far-\n",
    "        best_val_loss = epoch_val_loss\n",
    "        \n",
    "        # reset 'loc_patience' variable-\n",
    "        loc_patience = 0\n",
    "        \n",
    "        print(f\"\\nSaving model with lowest val_loss = {epoch_val_loss:.4f}\")\n",
    "        \n",
    "        # Save trained model with validation accuracy-\n",
    "        # torch.save(model.state_dict, f\"LeNet-300-100_Trained_{val_acc}.pth\")\n",
    "        torch.save(best_model.state_dict(), \"LeNet-300-100_Trained.pth\")\n",
    "        \n",
    "    else:  # there is no improvement in monitored metric 'val_loss'\n",
    "        loc_patience += 1  # number of epochs without any improvement\n",
    "\n",
    "\n",
    "    training_acc.append(epoch_acc * 100)\n",
    "    validation_acc.append(val_acc)\n",
    "    training_loss.append(epoch_loss)\n",
    "    validation_loss.append(epoch_val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6916c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4f49673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a LeNet-300-100 model to contain best weights from above-\n",
    "best_model = LeNet300()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95858264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights-\n",
    "best_model.load_state_dict(torch.load(\"/home/arjun/Documents/Programs/Python_Codes/PyTorch_Resources/Good_Codes/LeNet-300-100_Trained.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9073b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute performance of trained model on validation dataset-\n",
    "running_loss_val, correct, total = test_model(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b6b5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = running_loss_val / len(test_dataset)\n",
    "val_acc = 100 * (correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "300abac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained LeNet-300-100 dense neural network's performance on validation data:\n",
      "val_loss = 0.0979, val_accuracy = 97.89%\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained LeNet-300-100 dense neural network's performance on validation data:\")\n",
    "print(f\"val_loss = {val_loss:.4f}, val_accuracy = {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd5964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "800a7c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ef187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead398df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54736dba",
   "metadata": {},
   "source": [
    "## Implement Neural Network Pruning using _'torch.nn.utils.prune'_ module\n",
    "\n",
    "\n",
    "The available pruning methods:\n",
    "\n",
    "The following child classes inherit from the _BasePruningMethod_:\n",
    "\n",
    "- _torch.nn.utils.prune.Identity_: utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones\n",
    "\n",
    "- _torch.nn.utils.prune.RandomUnstructured_: prune (currently unpruned) entries in a tensor at random\n",
    "\n",
    "- _torch.nn.utils.prune.L1Unstructured_: prune (currently unpruned) entries in a tensor by zeroing out the ones with the lowest absolute magnitude\n",
    "\n",
    "- _torch.nn.utils.prune.RandomStructured_: prune entire (currently unpruned) rows or columns in a tensor at random\n",
    "\n",
    "- _torch.nn.utils.prune.LnStructured_: prune entire (currently unpruned) rows or columns in a tensor based on their Ln-norm (supported values of n correspond to supported values for argument p in torch.norm())\n",
    "- _torch.nn.utils.prune.CustomFromMask_: prune a tensor using a user-provided mask.\n",
    "\n",
    "\n",
    "Their 'functional equivalents' are:\n",
    "\n",
    "- _torch.nn.utils.prune.identity_\n",
    "\n",
    "- _torch.nn.utils.prune.random_unstructured_\n",
    "\n",
    "- _torch.nn.utils.prune.l1_unstructured_\n",
    "\n",
    "- _torch.nn.utils.prune.random_structured_\n",
    "\n",
    "- _torch.nn.utils.prune.ln_structured_\n",
    "\n",
    "- _torch.nn.utils.prune.custom_from_mask_\n",
    "\n",
    "Global pruning, in which entries are compared across multiple tensors, is enabled through \"torch.nn.utils.prune.global unstructured\"\n",
    "\n",
    "\n",
    "Refer to the research paper \"Streamlining Tensor and Network Pruning in PyTorch\" by Michela Paganini et al. for more details and [PyTorch Pruning Tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#iterative-pruning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb7219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca0b60bd",
   "metadata": {},
   "source": [
    "### Pruning multiple parameters in a model:\n",
    "\n",
    "By specifying the desired pruning technique and parameters, we can easily prune multiple tensors in a neural network, perhaps according to their type, as can be seen in this example-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac1d2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune multiple parameters/layers in a given model-\n",
    "for name, module in best_model.named_modules():\n",
    "    '''\n",
    "    # prune 15% of connections in all 2D-conv layers\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module = module, name = 'weight', amount = 0.15)\n",
    "    '''\n",
    "    # prune 20% of weights/connections in for all hidden layaers-\n",
    "    if isinstance(module, torch.nn.Linear) and name != 'output':\n",
    "        prune.l1_unstructured(module = module, name = 'weight', amount = 0.2)\n",
    "    \n",
    "    # prune 10% of weights/connections for output layer-\n",
    "    elif isinstance(module, torch.nn.Linear) and name == 'output':\n",
    "        prune.l1_unstructured(module = module, name = 'weight', amount = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8a44b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fc1.weight_mask', 'fc2.weight_mask', 'output.weight_mask'])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: verify that all of the defined pruning exists as masks-\n",
    "print(dict(best_model.named_buffers()).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2e93f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_params = count_params(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e2c37c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(266610)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_params.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12487b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546bcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea7502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef0184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7639a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input parameters for Early Stopping in manual implementation-\n",
    "minimum_delta = 0.001\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d147e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters for Early Stopping manual implementation-\n",
    "best_val_loss = 100\n",
    "loc_patience = 0\n",
    "\n",
    "# running_loss = 0.0\n",
    "# running_corrects = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0705a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 lists to store model training metrics-\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "training_loss = []\n",
    "validation_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d92437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "81c8e8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1 training loss = 0.0266, training accuracy = 99.11%, val_loss = 0.0980 & val_accuracy = 97.94%\n",
      "Number of parameters = 266610\n",
      "\n",
      "\n",
      "Saving model with lowest val_loss = 0.0980\n",
      "\n",
      "epoch: 2 training loss = 0.0266, training accuracy = 99.11%, val_loss = 0.0980 & val_accuracy = 97.94%\n",
      "Number of parameters = 266610\n",
      "\n",
      "\n",
      "epoch: 3 training loss = 0.0266, training accuracy = 99.11%, val_loss = 0.0980 & val_accuracy = 97.94%\n",
      "Number of parameters = 266610\n",
      "\n",
      "\n",
      "epoch: 4 training loss = 0.0266, training accuracy = 99.11%, val_loss = 0.0980 & val_accuracy = 97.94%\n",
      "Number of parameters = 266610\n",
      "\n",
      "\n",
      "epoch: 5 training loss = 0.0266, training accuracy = 99.11%, val_loss = 0.0980 & val_accuracy = 97.94%\n",
      "Number of parameters = 266610\n",
      "\n",
      "\n",
      "epoch: 6 training loss = 0.0266, training accuracy = 99.11%, val_loss = 0.0980 & val_accuracy = 97.94%\n",
      "Number of parameters = 266610\n",
      "\n",
      "\n",
      "'EarlyStopping' called!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop-\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    \n",
    "    if loc_patience >= patience:\n",
    "        print(\"\\n'EarlyStopping' called!\\n\")\n",
    "        break\n",
    "\n",
    "    running_loss, running_corrects = train_model(best_model, train_loader)\n",
    "  \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "    # epoch_acc = 100 * running_corrects / len(trainset)\n",
    "    # print(f\"\\nepoch: {epoch + 1} training loss = {epoch_loss:.4f}, training accuracy = {epoch_acc * 100:.2f}%\\n\")\n",
    "\n",
    "    running_loss_val, correct, total = test_model(best_model, test_loader)\n",
    "\n",
    "    epoch_val_loss = running_loss_val / len(test_dataset)\n",
    "    val_acc = 100 * (correct / total)\n",
    "    # print(f\"\\nepoch: {epoch + 1} training loss = {epoch_loss:.4f}, training accuracy = {epoch_acc * 100:.2f}%, val_loss = {epoch_val_loss:.4f} & val_accuracy = {val_acc:.2f}%\\n\")\n",
    "\n",
    "    print(f\"\\nepoch: {epoch + 1} training loss = {epoch_loss:.4f}, training accuracy = {epoch_acc * 100:.2f}%, val_loss = {epoch_val_loss:.4f} & val_accuracy = {val_acc:.2f}%\")\n",
    "\n",
    "    curr_params = count_params(best_model)\n",
    "    print(f\"Number of parameters = {curr_params}\\n\")\n",
    "    \n",
    "    percentage_pruned = ((orig_params - curr_params.numpy()) / orig_params * 100).numpy()\n",
    "    \n",
    "    # Code for manual Early Stopping:\n",
    "    # if np.abs(epoch_val_loss < best_val_loss) >= minimum_delta:\n",
    "    if (epoch_val_loss < best_val_loss) and np.abs(epoch_val_loss - best_val_loss) >= minimum_delta:\n",
    "        # print(f\"epoch_val_loss = {epoch_val_loss:.4f}, best_val_loss = {best_val_loss:.4f}\")\n",
    "        \n",
    "        # update 'best_val_loss' variable to lowest loss encountered so far-\n",
    "        best_val_loss = epoch_val_loss\n",
    "        \n",
    "        # reset 'loc_patience' variable-\n",
    "        loc_patience = 0\n",
    "        \n",
    "        print(f\"\\nSaving model with lowest val_loss = {epoch_val_loss:.4f}\")\n",
    "        \n",
    "        # Save trained model with validation accuracy-\n",
    "        # torch.save(model.state_dict, f\"LeNet-300-100_Trained_{val_acc}.pth\")\n",
    "        torch.save(best_model.state_dict(), f\"LeNet-300-100_{percentage_pruned:.2f}.pth\")\n",
    "        \n",
    "    else:  # there is no improvement in monitored metric 'val_loss'\n",
    "        loc_patience += 1  # number of epochs without any improvement\n",
    "\n",
    "    '''\n",
    "    training_acc.append(epoch_acc * 100)\n",
    "    validation_acc.append(val_acc)\n",
    "    training_loss.append(epoch_loss)\n",
    "    validation_loss.append(epoch_val_loss)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6207c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
